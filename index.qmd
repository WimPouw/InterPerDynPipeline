---
title: "An Open-source Standardized Pipeline for Tracing the Behavioral Dynamics in Social Interactions"
subtitle: "Computationally Reproducible Extended Methods and Results Section"
date: "`r format(Sys.time(), '%B %d, %Y')`"
author: 
  - name: Arkadiusz BiaÅ‚ek
    affiliation: Jagiellonian University, Poland
  - name: Wim Pouw
    affiliation: Tilburg University, Netherlands
  - name: James Trujillo
    affiliation: University of Amsterdam, Netherlands
  - name: Fred Hasselman
    affiliation: Radboud University, Netherlands
  - name: Babajide Alamu Owoyele
    affiliation: Hasso Plattner Institute, University of Potsdam, Germany
  - name: Natalia Siekiera
    affiliation: Jagiellonian University, Poland
  - name: Joanna RÄ…czaszek-Leonardi
    affiliation: University of Warsaw, Poland
  - name: Travis J. Wiltshire
    affiliation: Tilburg University, Netherlands
contact: 
  - name: Wim Pouw
    email: w.pouw@tilburguniversity.edu
bibliography: quarto_dependencies/references/refs.bib
theme: journal
css: quarto_dependencies/styles/styles.css
format:
  html:
    toc: true
    toc-location: left
    toc-title: "Contents"
    toc-depth: 3
    number-sections: true
    code-fold: true
    code-tools: true
---
![](images/ts.gif)

# Overview

To increase reproducibility of our approach we have designed this Quarto notebook that provides the user with what is effectively a manual-style extended methods and results section. Next to staging and (visually) explaining key Python and R code elements, the notebook contains detailed further â€œDo It Yourselfâ€ (DIY) instructions that can be accessed on demand by clicking on DIY sections, for example providing the technical steps needed to install and then run code for YOLO tracking on (user-defined) data. This notebook is therefore not simply an add-on supplemental text, but it is a key part of our contribution as it allows for newcomers to social signal processing to start implement this pipeline step by step.

The Quarto notebook is structured to have the following components, next to traditional text sections:

- ðŸš© **pipeline code** that overview complete code associated with the pipeline, or shorter code chunks to for example explain a particular function. These code blocks are generally not run inside the current notebook. Denoted with the symbol ðŸš©.

- ðŸ´ **other code chuncks** that we use for this quarto notebook to create sanity checks or animations related to measures or summarizing datasets. These code blocks are generally run inside the notebook. Denoted with the symbol ðŸ´.

- ðŸ“Š **example output** that shows example output of the pipeline. Denoted with the symbol ðŸ“Š.

- ðŸ› ï¸ **DIY blocks** that provide shorter, step-by-step links to the full code for executing a particular step in the pipeline on your own data.  Denoted with the symbol ðŸ› ï¸.

- â¬‡ï¸ **dataset download** provide information about how to download the dataset overviewed for our report. Denoted with the symbol â¬‡ï¸.

**What the Quarto notebook is not intended to do:** This notebook is not itself the pipeline (i.e., a collection of code that runs the complete pipeline). It is rather a linear step-by-step manual and explainer of how to understand and implement the pipeline steps. For actually applying this code, the user can further dig into the **DIY** blocks. 

# Dataset
Below is a description of the two datasets that were used as test cases for the pipeline. The datasets are available in masked form to reduce identifiability of the individuals in the dataset. Following the download guide for further information on how to access the datasets.

::: {.callout-note .callout-download collapse="true"}
## Download Guide â¬‡ï¸
The masked data can be downloaded from the repository of Jagellonian University [here](). The unmasked data can be requested from the authors.

Please note that the masked data is not the input of the pipeline. YOLO tracking would not work well on masked data as detailed information about the body is lost, and only represented as the motion tracking data that comes with it (and is present in the repository). Rather the masked data can be used to check for qualitative changes in the behavior of the inviduals, and can be used to create animations such as in step 1.3.

**Example of masked videodata**
![](./images/masked_example.jpeg){fig-align="center" width=600}
:::

### Dataset Mother Infant
Thirteen infant-caregiver pairs visited the child laboratory monthly for a total of seven visits. The infants' ages at the first visit ranged from 7 to 9 months. Parents were instructed to "play as usual," with a set of different toys to encourage spontaneous interactions. Each visit lasted approximately 30 minutes and was divided into four stages. For our analyses, we focus on recordings from the stage where the infant was seated in a feeding chair, facing the caregiver sitting directly across. This controlled setup minimizes movement around the room, facilitating video motion tracking analysis. Additionally, our analyses are restricted to top-view recordings, ensuring that both the caregiver and the infant are captured within a single frame.

::: {.callout-note .callout-download collapse="true"}
## Plotting code for the Mother-Infant dataset ðŸ´

```{python, eval=FALSE}

import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt
plt.ioff()
plt.clf() # Clear any existing figures

import pandas as pd
import numpy as np
import os

# Load the data
df = pd.read_csv('./meta/project_point_metadata_ages.csv')

# Convert gender to categorical label
df['gender_label'] = df['gender'].map({0: 'Female', 1: 'Male'})

# Create a 2x2 subplot layout
fig, axs = plt.subplots(2, 2, figsize=(12, 10), gridspec_kw={'width_ratios': [1.5, 1], 'height_ratios': [1.5, 1]})
fig.suptitle('Mother-Infant Dataset Overview', fontsize=16, fontfamily='serif')

# Define colors for gender
colors = {
    'Female': '#C55A11',   # Orange for females
    'Male': '#4472C4'       # Blue for males
}

# 1. Line plot - Age progression across time points
for idx, row in df.iterrows():
    gender = row['gender_label']
    # Create arrays for days and time points, handling NaN values
    days = []
    timepoints = []

    for i in range(1, 8):   # T1 to T7
        day_col = f'age_T{i}_days'
        if day_col in df.columns and pd.notna(row[day_col]) and row[day_col] > 0:
            days.append(row[day_col])
            timepoints.append(i)

    # Plot the line for this infant
    axs[0, 0].plot(
        timepoints,
        days,
        marker='o',
        color=colors[gender],
        alpha=0.5,
        linewidth=1.5
    )

# Customize the plot
axs[0, 0].set_xlabel('Time Point', fontfamily='serif')
axs[0, 0].set_ylabel('Age (days)', fontfamily='serif')
axs[0, 0].set_title('Infant Age Progression', fontfamily='serif')
axs[0, 0].set_xticks(range(1, 8))
axs[0, 0].set_xticklabels([f'T{i}' for i in range(1, 8)])
axs[0, 0].grid(True, alpha=0.3)

# Add legend patches
from matplotlib.patches import Patch
legend_elements = [
    Patch(facecolor=colors['Female'], edgecolor='w', label='Female'),
    Patch(facecolor=colors['Male'], edgecolor='w', label='Male')
]
axs[0, 0].legend(handles=legend_elements, loc='upper left')

# 2. Box plot - Age distribution at each time point
time_point_data = []
labels = []

for i in range(1, 8):   # T1 to T7
    col = f'age_T{i}_days'
    if col in df.columns:
        valid_data = df[col].dropna()
        if len(valid_data) > 0:
            time_point_data.append(valid_data)
            labels.append(f'T{i}')

# Create violin plots
if time_point_data:
    violin_parts = axs[0, 1].violinplot(
        time_point_data,
        showmeans=True,
        showmedians=True
    )

    # Color the violin plots
    for i, pc in enumerate(violin_parts['bodies']):
        pc.set_facecolor('#7030A0')  # Purple
        pc.set_edgecolor('black')
        pc.set_alpha(0.7)

# Set labels
axs[0, 1].set_xticks(range(1, len(labels) + 1))
axs[0, 1].set_xticklabels(labels)
axs[0, 1].set_xlabel('Time Point', fontfamily='serif')
axs[0, 1].set_ylabel('Age (days)', fontfamily='serif')
axs[0, 1].set_title('Age Distribution by Time Point', fontfamily='serif')
axs[0, 1].grid(True, alpha=0.3, axis='y')

# 3. Pie chart - Gender distribution
gender_counts = df['gender_label'].value_counts()
axs[1, 0].pie(
    gender_counts,
    labels=gender_counts.index,
    autopct='%1.1f%%',
    colors=[colors[g] for g in gender_counts.index],
    wedgeprops={'edgecolor': 'w', 'linewidth': 1}
)
axs[1, 0].set_title('Gender Distribution', fontfamily='serif')

# 4. Summary table
# Calculate averages for each time point, handling NaN values
averages_days = []
for i in range(1, 8):
    col = f'age_T{i}_days'
    if col in df.columns:
        avg = df[col].mean()
        if not pd.isna(avg):
            avg_months = avg / 30.44  # Convert to months
            averages_days.append([f"T{i} Average Age", f"{avg:.1f} days ({avg_months:.1f} months)"])

# Count participants at each time point
participants = []
for i in range(1, 8):
    col = f'age_T{i}_days'
    if col in df.columns:
        count = df[col].notna().sum()
        participants.append([f"Participants at T{i}", f"{count}"])

# Overall statistics
summary_data = [
    ["Total Infants", f"{len(df)}"],
    ["Females", f"{(df['gender'] == 0).sum()}"],
    ["Males", f"{(df['gender'] == 1).sum()}"]
]

# Add age ranges if columns exist
if 'age_T1_days' in df.columns:
    summary_data.append(["Age Range T1", f"{df['age_T1_days'].min():.0f}-{df['age_T1_days'].max():.0f} days"])

last_tp = 7
while last_tp > 0:
    col = f'age_T{last_tp}_days'
    if col in df.columns and df[col].notna().sum() > 0:
        summary_data.append([f"Age Range T{last_tp}", f"{df[col].min():.0f}-{df[col].max():.0f} days"])
        break
    last_tp -= 1

# Create the table with the most important summary statistics
axs[1, 1].axis('off')
table = axs[1, 1].table(
    cellText=summary_data + participants,
    loc='center',
    cellLoc='left'
)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 1.5)
axs[1, 1].set_title('Dataset Summary', fontfamily='serif')

plt.tight_layout()
plt.subplots_adjust(top=0.9)

# Save the figure to a file
output_path = "images/mother_infant_analysis.png"
os.makedirs(os.path.dirname(output_path), exist_ok=True)
plt.savefig(output_path, dpi=300, bbox_inches='tight')
plt.close(fig) # Explicitly close the figure object
```

:::

![](images/mother_infant_analysis.png){fig-align="center" width=600}

Note that in our github repository we use short sample data from this dataset from an infant-mother pair whom we are allowed to share the video data for the current purposes.

### Dataset Siblings
Ten sibling pairs (total = 20 pairs) were observed from Polish urban culture and YurakarÃ© indigenous culture each. Observations were made during a semi-structured play situation: in the natural environment of the YurakarÃ© community and laboratory conditions in Poland. Siblings were asked to build a tower together using 54 wooden blocks while sitting facing each other. Each pair had 4 minutes to complete the task. This setup allowed for a systematic comparison of coordinated and mutually adjusted movements in siblings, using our pipeline to examine the smoothness of interaction while "doing things together" in these two cultural groups.

::: {.callout-note .callout-download collapse="true"}
## Plotting code for the Siblings dataset ðŸ´
```{python, eval=FALSE, code_folding="hide"}
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os

# Load the data
df = pd.read_csv('./meta/project_siblings_metadata_ages_gender.csv')

# Data preprocessing
df['AgeDifference'] = abs(df['P1agedays'] - df['P2agedays'])
df['AverageAge'] = (df['P1agedays'] + df['P2agedays']) / 2
df['GenderCombo'] = df['GenderP1'] + '-' + df['GenderP2']

# Create a 2x2 subplot layout
fig, axs = plt.subplots(2, 2, figsize=(10, 8), gridspec_kw={'width_ratios': [1.5, 1], 'height_ratios': [1.5, 1]})
fig.suptitle('Sibling Dataset Overview', fontsize=16, fontfamily='serif')

# Define colors for gender combinations
colors = {
    'M-M': '#4472C4',   # Blue
    'M-F': '#7030A0',   # Purple
    'F-M': '#548235',   # Green
    'F-F': '#C55A11'    # Rust/Orange
}

# 1. Scatter plot - P1 age vs P2 age
for gender_combo, group in df.groupby('GenderCombo'):
    axs[0, 0].scatter(
        group['P1agedays'],
        group['P2agedays'],
        color=colors.get(gender_combo, 'gray'),
        alpha=0.7,
        label=gender_combo,
        s=50
    )

# Add reference line
min_age = min(df['P1agedays'].min(), df['P2agedays'].min())
max_age = max(df['P1agedays'].max(), df['P2agedays'].max())
axs[0, 0].plot([min_age, max_age], [min_age, max_age], 'k--', alpha=0.3)
axs[0, 0].set_xlabel('P1 Age (days)', fontfamily='serif')
axs[0, 0].set_ylabel('P2 Age (days)', fontfamily='serif')
axs[0, 0].set_title('Participant Ages (P1 vs P2)', fontfamily='serif')
axs[0, 0].grid(True, alpha=0.3)
axs[0, 0].legend()

# 2. Bar chart - Age differences
bar_positions = np.arange(len(df))
bar_width = 0.8
axs[0, 1].bar(
    bar_positions,
    df['AgeDifference'],
    width=bar_width,
    color=[colors.get(combo, 'gray') for combo in df['GenderCombo']]
)
axs[0, 1].set_xticks(bar_positions)
axs[0, 1].set_xticklabels(df['Code'], rotation=90)
axs[0, 1].set_xlabel('Sibling Pair', fontfamily='serif')
axs[0, 1].set_ylabel('Age Difference (days)', fontfamily='serif')
axs[0, 1].set_title('Age Differences by Sibling Pair', fontfamily='serif')
axs[0, 1].grid(True, alpha=0.3, axis='y')

# 3. Pie chart - Gender distribution
gender_counts = df['GenderCombo'].value_counts()
axs[1, 0].pie(
    gender_counts,
    labels=gender_counts.index,
    autopct='%1.1f%%',
    colors=[colors.get(combo, 'gray') for combo in gender_counts.index],
    wedgeprops={'edgecolor': 'w', 'linewidth': 1}
)
axs[1, 0].set_title('Gender Distribution', fontfamily='serif')

# 4. Summary table
summary_data = [
    ["Total Sibling Pairs", f"{len(df)}"],
    ["Average P1 Age", f"{df['P1agedays'].mean()/30:.1f} months"],
    ["Average P2 Age", f"{df['P2agedays'].mean()/30:.1f} months"],
    ["Average Age Difference", f"{df['AgeDifference'].mean()/30:.1f} months"],
    ["Number of Males", f"{(df['GenderP1'] == 'M').sum() + (df['GenderP2'] == 'M').sum()}"],
    ["Number of Females", f"{(df['GenderP1'] == 'F').sum() + (df['GenderP2'] == 'F').sum()}"]
]

# Turn off axis for table
axs[1, 1].axis('off')
table = axs[1, 1].table(
    cellText=[row for row in summary_data],
    colLabels=["Measure", "Value"],
    loc='center',
    cellLoc='left'
)
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 1.5)
axs[1, 1].set_title('Dataset Summary', fontfamily='serif')

plt.tight_layout()
plt.subplots_adjust(top=0.9)

# Save the figure to a file
output_path = "images/sibling_analysis.png"
os.makedirs(os.path.dirname(output_path), exist_ok=True)
plt.savefig(output_path, dpi=300, bbox_inches='tight')
plt.close()
```
:::

![](images/sibling_analysis.png){fig-align="center" width=600}

::: {.callout-note .callout-settingup collapse="true"}
## DIY: Setting up the repository on your local machine ðŸ› ï¸

### Step 0: Github repo
You should first clone the repository [InterPerDynPipeline](https://github.com/WimPouw/InterPerDynPipeline) and move to the root directory. The step 1 code for tracking is in the `./code_STEP1_posetrackingprocessing/` folder. In this folder you will find the Python script `yolo_tracking_processing.py`. To check whether you have the correct script you can compare against the code chunk provided below.

First install git if you do not have it installed yet. You can find instructions on how to install git [here](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).

Then you can clone the repository using the following commands in your (anaconda) terminal:

```bash
git clone https://github.com/WimPouw/InterPerDynPipeline
cd ./InterPerDynPipeline/
```

:::

# Step 1: Motion tracking and signal processing and wrangling

The motion tracking and signal processing pipeline is divided into three steps: 
1. **Tracking**: This step involves tracking the movements of individuals in the videos using YOLOv8. The output is a video with annotated keypoints and a CSV file containing the coordinates of these keypoints. 2. **Signal Processing**: This step involves processing the keypoint data to extract relevant features for analysis. The output are processed timeseries files, and a flat dataset containing our smoothness measures that are directly ready for statistical analysis. 3. **Animations**: This step involves animating the processed data together with the original video data to ensure that the processed data are of sufficient quality to reflect behavioral changes.


## Step 1.1: Tracking two persons in videos for top-view (Python)
We have tried several pose tracking solutions, such as OpenPose (model 25B; @caoOpenPoseRealtimeMultiPerson2021a), Mediapipe (default highest complexity blazepose model; @lugaresiMediaPipeFrameworkBuilding2019). We found that these models are not well equipped to track persons from top view, most likely because these models are not trained on ground-truth poses of persons from top-view camera angles. However, we found the heaviest model of YOLOv8 ("yolov8x-pose-p6.pt") does perform very well, especially as compared to the other models we tested. Thus, for this pipeline we recommend using YOLOv8 model [@jocherYOLOUltralytics2023] for top-view tracking.

::: {.callout-note .callout-installation collapse="true"}
## DIY Step 1.1 Motion Tracking Set-Up and Execution ðŸ› ï¸

### Step 1: Install requirements
For each script we have provided a `requirements.txt` file. You can install the requirements using pip, by first navigating to the folder where the script is located and then running the following command in your terminal:

```bash
cd [yourspecificrootadress]/code_STEP1_posetrackingprocessing/
pip install -r requirements.txt
```

### Step 2: Download the YOLOv8 model
In the current GitHub repo we have a light-weight model `yolov8n-pose.pt` (~6.5MB) for immediate testing of the code. However, this model is not as accurate as the heaviest model.

We should download the heaviest model (`yolov8x-pose-p6.pt`) and save it to the `./code_STEP1_posetrackingprocessing/model/` directory:  
https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8x-pose-p6.pt

**Note** that if you're running the heavyweight model, you will need GPU support for this to run in a reasonable time (we processed our data on a NVIDIA GeForce RTX 4090 GPU). The lightweight model can run on CPU.


### Step 3: Run the python script
Assuming you have videos in your data folder (`./data_raw/`) and you have a `.pt` model in the model folder, you can run the script using the following command:

```bash
cd [yourspecificrootadress]/code_STEP1_posetrackingprocessing/
python yolo_tracking_processing.py
```
:::

The code below is the script `yolo_tracking_processing.py` that you can run to track the videos. The output will be a video with annotated keypoints and a CSV file containing the coordinates of these keypoints. The script processes each video frame-by-frame, detecting people and their pose keypoints (17 points as listed in the script; see the `GetKeypoint class`). We filter out duplicate detections or skeletons with excessive missing data. Specifically, for each processed video, the script generates two output files: an annotated video showing the original footage with skeleton overlays (green points for accepted keypoints, red for filtered ones, and blue lines connecting the points), and a CSV file containing the raw coordinate data (frame number, person ID, keypoint ID, x-coordinate, y-coordinate). The output filenames include parameters like "c150" (150px proximity threshold) and "miss95" (95% missing data tolerance), which control how the system handles potential duplicate detections and incomplete skeletons. The user could adjust the parameter `close_threshold` and `miss_tolerance` to adjust the detection dynamics. We output the annotated video and the CSV file in the `./datatracked_afterSTEP1/` folder.


::: {.callout-note .callout-installation collapse="true"}
## Pipeline code 1.1 YOLO tracking ðŸš©

```{python, eval=FALSE, code_folding="hide"}
from ultralytics import YOLO
from pydantic import BaseModel
import cv2
import csv
import numpy as np
import glob
import os
import torch # for gpu support
from itertools import combinations
import sys

torch.cuda.set_device(0)
# Load the model
modelfolder = './model/'
modellocation = glob.glob(modelfolder+"*.pt")[0]
modelfile = os.path.basename(modellocation)
print(f"We are loading in the following YOLO model: {modelfile}")
model = YOLO(modellocation)
# main variables
video_folder = "../data_raw/"
# avi mp4 or other video formats
video_files = glob.glob(video_folder + "*.mp4") + glob.glob(video_folder + "*.avi") + glob.glob(video_folder + "*.mov") + glob.glob(video_folder + "*.mkv")
step1resultfolder = "../datatracked_afterSTEP1/"
print(video_files)

# keypoint names
class GetKeypoint(BaseModel):
    NOSE:           int = 0
    LEFT_EYE:       int = 1
    RIGHT_EYE:      int = 2
    LEFT_EAR:       int = 3
    RIGHT_EAR:      int = 4
    LEFT_SHOULDER:  int = 5
    RIGHT_SHOULDER: int = 6
    LEFT_ELBOW:     int = 7
    RIGHT_ELBOW:    int = 8
    LEFT_WRIST:     int = 9
    RIGHT_WRIST:    int = 10
    LEFT_HIP:       int = 11
    RIGHT_HIP:      int = 12
    LEFT_KNEE:      int = 13
    RIGHT_KNEE:     int = 14
    LEFT_ANKLE:     int = 15
    RIGHT_ANKLE:    int = 16
get_keypoint = GetKeypoint()


# Define skeleton connections
skeleton = [
    (get_keypoint.LEFT_SHOULDER, get_keypoint.RIGHT_SHOULDER),
    (get_keypoint.LEFT_SHOULDER, get_keypoint.LEFT_ELBOW),
    (get_keypoint.RIGHT_SHOULDER, get_keypoint.RIGHT_ELBOW),
    (get_keypoint.LEFT_ELBOW, get_keypoint.LEFT_WRIST),
    (get_keypoint.RIGHT_ELBOW, get_keypoint.RIGHT_WRIST),
    (get_keypoint.LEFT_SHOULDER, get_keypoint.LEFT_HIP),
    (get_keypoint.RIGHT_SHOULDER, get_keypoint.RIGHT_HIP),
    (get_keypoint.LEFT_HIP, get_keypoint.RIGHT_HIP),
    (get_keypoint.LEFT_HIP, get_keypoint.LEFT_KNEE),
    (get_keypoint.RIGHT_HIP, get_keypoint.RIGHT_KNEE),
    (get_keypoint.LEFT_KNEE, get_keypoint.LEFT_ANKLE),
    (get_keypoint.RIGHT_KNEE, get_keypoint.RIGHT_ANKLE),
]

def tensor_to_matrix(results_tensor):
    # this just takes the results output of YOLO and coverts it to a matrix,
    # making it easier to do quick calculations on the coordinates
    results_list = results_tensor.tolist()
    results_matrix = np.matrix(results_list)
    results_matrix[results_matrix==0] = np.nan
    return results_matrix

def check_for_duplication(results):
    # this threshold determines how close two skeletons must be in order to be
    # considered the same person. Arbitrarily chosen for now.
    close_threshold =150
    # missing data tolerance
    miss_tolerance = 0.95 # this means we can miss up to 75% of the keypoints
    drop_indices = []
    if len(results[0].keypoints.xy) > 1:
        conf_scores = []
        # get detection confidence for each skeleton
        for person in tensor_to_matrix(results[0].keypoints.conf):
            conf_scores.append(np.mean(person))
           
        # this list will stores which comparisons need to be made
        combos = list(combinations(range(len(results[0].keypoints.xy)), 2))

        # now loop through these comparisons
        for combo in combos:
            closeness = abs(np.nanmean(tensor_to_matrix(results[0].keypoints.xy[combo[0]]) - 
                        tensor_to_matrix(results[0].keypoints.xy[combo[1]])))
            # if any of them indicate that two skeletons are very close together,
            # we keep the one with higher tracking confidence, and remove the other
            if closeness < close_threshold:
                conf_list = [conf_scores[combo[0]], conf_scores[combo[1]]]
                idx_min = conf_list.index(min(conf_list))
        
                
                drop_indices.append(combo[idx_min])
                
        # additional checks:
        for person in range(len(results[0].keypoints.xy)):
           keypoints_missed =  np.isnan(tensor_to_matrix(results[0].keypoints.xy[person])).sum()/2
           perc_missed = keypoints_missed/len(tensor_to_matrix(results[0].keypoints.xy[person]))
           
           if perc_missed > miss_tolerance:
               drop_indices.append(person)
        
    return list(set(drop_indices))


for video_path in video_files:
    # Video path
    video_path = video_path
    # only if the output is not there yet
    if os.path.exists(step1resultfolder+ os.path.basename(video_path).split('.')[0]+"_annotated_layer1_c150_miss95.mp4"):
        print(f"Output video already exists for {video_path}. Skipping...")
        continue
    # vidname without extension
    vidname = os.path.basename(video_path)
    vidname = vidname.split('.')[0]

    # Open the video
    cap = cv2.VideoCapture(video_path)

    # Get video properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Define the output video writer
    corename = os.path.basename(video_path).split('.')[0]
    output_path = step1resultfolder+ vidname+"_annotated_layer1_c150_miss95.mp4"
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    # Prepare CSV file
    csv_path = step1resultfolder+ vidname+'_keypoints_data_layer1.csv'
    csv_file = open(csv_path, 'w', newline='')
    csv_writer = csv.writer(csv_file)

    # Write header
    header = ['frame', 'person', 'keypoint', 'x', 'y']
    csv_writer.writerow(header)

    frame_count = 0

    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            break
    
        # Run YOLOv8 inference on the frame
        results = model(frame)
               
        # write empty rows if no person is detected
        if len(results[0].keypoints.xy) == 0:
            csv_writer.writerow([frame_count, None, None, None, None])
        annotated_frame = frame
        
        # only do this if a person is detected
        if len(results[0].keypoints.xy) > 0:
            # Process the results
            drop_indices = []
            
            drop_indices = check_for_duplication(results)
            
            
            for person_idx, person_keypoints in enumerate(results[0].keypoints.xy):
                if person_idx not in drop_indices:
                    colourcode = (0, 255, 0)
                else:
                    colourcode = (255, 0, 0)
                
                for keypoint_idx, keypoint in enumerate(person_keypoints):
                    x, y = keypoint
 
                    # Write to CSV
                    csv_writer.writerow([frame_count, person_idx, keypoint_idx, x.item(), y.item()])
                    
                    # Draw keypoint on the frame
                    cv2.circle(annotated_frame, (int(x), int(y)), 5, colourcode, -1)
                
                # Draw skeleton
                for connection in skeleton:
                    if connection[0] < len(person_keypoints) and connection[1] < len(person_keypoints):
                        start_point = tuple(map(int, person_keypoints[connection[0]]))
                        end_point = tuple(map(int, person_keypoints[connection[1]]))
                        if all(start_point) and all(end_point):  # Check if both points are valid
                            cv2.line(annotated_frame, start_point, end_point, (255, 0, 0), 2)
        
            # Write the frame to the output video
            out.write(annotated_frame)
        
        frame_count += 1

    # Release everything
    cap.release()
    out.release()
    cv2.destroyAllWindows()
    csv_file.close()
    print(f"Output video saved as {output_path}")
    print(f"Keypoints data saved as {csv_path}")
```

:::

Here is an example of the output video with annotated keypoints and skeletons. The green points indicate accepted keypoints, while the red points indicate filtered ones. The blue lines connect the keypoints to form a skeleton.

<html>
<video width="600" height="400" controls>
  <source src="./images/videosrerendered/sample.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>
</html>

Here is an example of the output csv file containing the coordinates of the keypoints. The CSV file contains the following columns: frame number, person ID, keypoint ID, x-coordinate, and y-coordinate. Each row corresponds to a detected keypoint in a specific frame.

::: {.callout-note .callout-installation collapse="true"}
## Example output and code example for YOLO tracking: ðŸ“Š

```{python examplecsvfile}
import pandas as pd
import glob as glob
import os

folderstep1output = "./dataoutput_STEP1_1_rawposedata/"
# Load the CSV file
csv_file = glob.glob(os.path.join(folderstep1output + "*.csv"))[0]
df = pd.read_csv(csv_file)

# Display the first few rows of the DataFrame
print(df.head())
```

:::

## Step 1.2: Raw pose data post-processing for timeseries matrices (Python)
The raw pose data with frame, person, keypoint, and x,y  position data that we end up with is not yet in a format that is easy to work with for our purposes. Additionally, sometimes we dont have a person or two persons detected in the frame. We want to transform these raw pose data to timeseries matrix format, whereby we have time in seconds as one column and different time-varying variables in the other columns. Furthermore, we smooth and interpolate the data if necessary. The below python code executes this second step whereby we store the position data per person, and furthe derive interpersonal distances measures, and kinematic metrics (e.g. speed) and tracking quality indicators (e.g. tracking status that says something about where it concerns interpolated data).

Next to determining time in seconds from the frame rate of the videos, the script starts with assigning left and right person IDs based on the horizontal position in the frame (with left being ID 1 and right being ID 2). It then processes the time data to calculate statistics for tracked people using upper body keypoints (as the lower body keypoints are unreliable from top-view and may be ignored). Specifically, we reduce the dimensionality of all upper body keypoints by determined relative positions and their distances: We calculate a bounding box (determined by the outer edges of the keypoints) and determine the center in x and y coordinates (`centroid_x`, `centroid_y`) and their P1-P2 distance of those centroids (`distance`), and the center of mass (`com_x`, `com_y`) for each person and the distnace (`distance_com`). Further we determine the midpoint of the shoulders as another relevant position (`shoulder_midpoint_x`, `shoulder_midpoint_y`) and compute its distance (`distance_shoulder_midpoint`). Additionally, information about body parts such as arm motions by taking the wrist positions (`wrist_left_x`, `wrist_left_y`, `wrist_right_x`, `wrist_right_y`), and further deriving total motion per second, called speed (based on a euclidean sum of the changes of position along x and y). 

The Savitzky-Golay filter is applied for smoothing jittering position data (using a window size of 11 frames, polynomial order of 3). The script handles missing data through two complementary techniques: linear interpolation for brief gaps in tracking (up to 5 frames) and we take the last known location for longer gaps (up to 20 frames). The script also calculates the speed of the center of mass and wrist positions, and here we also smooth the values using the Savitzky-Golay filter to reduce noise that is amplified during differientation (see @winterBiomechanicsMotorControl2009).

The more sophisticated measure that we have created is the proximity calculation measure (see function `calculate_approach()` in the step2 script). This function establishes initial reference position for both participants and then projects their subsequent movements onto the connecting line between them. This allows for quantifying approach and retreat behaviors for each person seperately (which a distance measure does not allow for) while allowing for relative angle changes between them - positive values indicate movement toward the other person (relative from start), while negative values indicate withdrawal (relative from start).

Below is an animation to exemplify what we are measuring. We are here capturing only movements that are changing the distance between them. When someone moves in a circle around the other person we want to make sure we do not register that as approach or retreat (which we would do if we just capture horizontal position for example). However we also want capture each person their own approach/avoidance value (which would not be possible when using a simple distance measure). By always measuring the signed length of the projected line from the initial position (based on the current connected line), the method tracks cumulative approach/avoidance for each person over time. 

::: {.callout-note .callout-installation collapse="true"}
## Code for creating animation to exemplify behavior of proximity measure ðŸ´

```{python,}
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import matplotlib.patches as patches
import matplotlib.lines as mlines
import os
import matplotlib
# Force matplotlib to use Agg backend to avoid display issues
matplotlib.use('Agg')

# Create output directory if it doesn't exist
os.makedirs("images/proximity_visualization", exist_ok=True)

# Simulation parameters
num_frames = 100
p1_color = '#3498db'  # Blue
p2_color = '#e74c3c'  # Red
vector_color = '#2ecc71'  # Green

# Create initial positions for two people - now on a horizontal line
# Person 1 on the left, Person 2 on the right (same y-coordinate)
p1_initial = np.array([3, 5])
p2_initial = np.array([7, 5])

# Generate movement paths with MORE EXTREME approach and avoidance
# Person 1 will make a dramatic approach followed by retreat
# Person 2 will make a clear retreat followed by approach
t = np.linspace(0, 2*np.pi, num_frames)

# Create more dramatic movement path for Person 1
# Increase the amplitude of movement to make it more extreme
p1_path_x = p1_initial[0] + 2.5 * np.sin(t/2)  # Larger horizontal movement (was 1.5)
p1_path_y = p1_initial[1] + 2.5 * np.sin(t)   # Larger vertical movement (was 2.0)

# Create more dramatic movement path for Person 2
# Make the retreat phase more obvious
p2_path_x = p2_initial[0] - 1.2 * np.sin(t)    # More horizontal movement (was 0.5)
p2_path_y = p2_initial[1] - 2.0 * np.sin(t*1.5)  # More vertical movement (was 1.0)

# Store positions for each frame
p1_positions = np.column_stack((p1_path_x, p1_path_y))
p2_positions = np.column_stack((p2_path_x, p2_path_y))

# Arrays to store approach values
p1_approach_values = np.zeros(num_frames)
p2_approach_values = np.zeros(num_frames)

# Calculate proximity approach values
def calculate_approach(positions1, positions2):
    """Calculate approach values similar to the script's approach"""
    # Use first frame as reference
    reference_p1_pos = positions1[0].copy()
    reference_p2_pos = positions2[0].copy()
    
    p1_approach = np.zeros(len(positions1))
    p2_approach = np.zeros(len(positions2))
    
    for i in range(len(positions1)):
        # Current positions
        p1_pos = positions1[i]
        p2_pos = positions2[i]
        
        # Current connecting vector and direction
        current_connect = p2_pos - p1_pos
        current_distance = np.linalg.norm(current_connect)
        
        if current_distance > 0:
            current_direction = current_connect / current_distance
            
            # Vectors from reference positions
            p1_vector = p1_pos - reference_p1_pos
            p2_vector = p2_pos - reference_p2_pos
            
            # Project onto connecting line
            p1_approach[i] = np.dot(p1_vector, current_direction)
            
            # For P2, a positive value should mean movement toward P1
            # So we use the negative connecting direction (from P2 to P1)
            # And a positive dot product means approaching
            p2_direction = -current_direction  # Direction from P2 to P1
            p2_approach[i] = np.dot(p2_vector, p2_direction)
    
    return p1_approach, p2_approach

# Calculate approach values
p1_approach_values, p2_approach_values = calculate_approach(p1_positions, p2_positions)

# Create the main visualization
def create_detailed_animation():
    # Set up the figure explicitly with DPI
    fig, ax = plt.subplots(figsize=(12, 10), dpi=100)  # Larger figure for better visibility
    
    def init():
        ax.clear()
        ax.set_xlim(0, 10)
        ax.set_ylim(0, 10)
        ax.set_aspect('equal')
        return []

    def animate(i):
        ax.clear()
        
        # Set up the axes
        ax.set_xlim(0, 10)
        ax.set_ylim(0, 10)
        ax.set_aspect('equal')
        ax.set_title('Proximity Approach Visualization (Extreme Movements)', fontsize=16)
        ax.set_xlabel('X Position', fontsize=12)
        ax.set_ylabel('Y Position', fontsize=12)
        
        # Get current positions
        p1_pos = p1_positions[i]
        p2_pos = p2_positions[i]
        
        # Draw the initial connecting vector (reference)
        ax.plot([p1_positions[0][0], p2_positions[0][0]], 
                [p1_positions[0][1], p2_positions[0][1]], 
                color=vector_color, linewidth=1, linestyle='--', alpha=0.5, zorder=0)
        
        # Draw the current connection vector
        ax.plot([p1_pos[0], p2_pos[0]], [p1_pos[1], p2_pos[1]], 
                 color=vector_color, linewidth=2.5, linestyle='-', zorder=1)
        
        # Draw person markers
        ax.scatter(p1_pos[0], p1_pos[1], s=180, color=p1_color, edgecolor='black', linewidth=1.5, zorder=3)
        ax.scatter(p2_pos[0], p2_pos[1], s=180, color=p2_color, edgecolor='black', linewidth=1.5, zorder=3)
        
        # Draw initial positions
        ax.scatter(p1_positions[0][0], p1_positions[0][1], s=90, color=p1_color, 
                   alpha=0.5, edgecolor='black', linewidth=1, zorder=2)
        ax.scatter(p2_positions[0][0], p2_positions[0][1], s=90, color=p2_color, 
                   alpha=0.5, edgecolor='black', linewidth=1, zorder=2)
        
        # Draw trails (last 15 positions)
        start_idx = max(0, i-15)
        ax.plot(p1_positions[start_idx:i+1, 0], p1_positions[start_idx:i+1, 1], 
                '-', color=p1_color, alpha=0.5, linewidth=2)
        ax.plot(p2_positions[start_idx:i+1, 0], p2_positions[start_idx:i+1, 1], 
                '-', color=p2_color, alpha=0.5, linewidth=2)
        
        # Visualize the approach values with text - make them larger and more visible
        approach_text = (f"P1 approach: {p1_approach_values[i]:.2f}\n"
                        f"P2 approach: {p2_approach_values[i]:.2f}")
        ax.text(0.05, 0.95, approach_text, transform=ax.transAxes, 
                verticalalignment='top', fontsize=14, fontweight='bold',
                bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))
        
        # Add projection visualization
        if i > 0:
            # Get current connecting direction
            connect_vector = p2_pos - p1_pos
            connect_distance = np.linalg.norm(connect_vector)
            connect_direction = connect_vector / connect_distance
            
            # Person 1 projection
            p1_vector = p1_pos - p1_positions[0]
            p1_projection = np.dot(p1_vector, connect_direction) * connect_direction
            
            # Draw projection line for P1
            p1_proj_point = p1_positions[0] + p1_projection
            
            # Person 2 projection - use direction from P2 to P1
            p2_vector = p2_pos - p2_positions[0]
            p2_direction = -connect_direction  # Direction from P2 to P1
            p2_projection = np.dot(p2_vector, p2_direction) * p2_direction
            
            # Draw projection line for P2
            p2_proj_point = p2_positions[0] + p2_projection
            
            # Draw projection lines with arrows to show direction
            # For P1
            p1_approach = p1_approach_values[i]
            if abs(p1_approach) > 0.1:  # Only draw if there's a significant approach value
                # Draw line with increased width
                ax.plot([p1_positions[0][0], p1_proj_point[0]], 
                        [p1_positions[0][1], p1_proj_point[1]], 
                        '--', color=p1_color, linewidth=2.5)
                
                # Add arrow to show direction - make arrow larger
                arrow_length = min(abs(p1_approach) * 0.2, 0.6)  # Scale arrow size
                arrow_width = 0.15
                if p1_approach > 0:  # Approaching
                    # Arrow pointing from initial position toward projection point
                    ax.arrow(p1_positions[0][0], p1_positions[0][1], 
                            arrow_length * connect_direction[0], arrow_length * connect_direction[1],
                            head_width=arrow_width, head_length=arrow_width*1.5, 
                            fc=p1_color, ec='black', linewidth=1, zorder=4)
                else:  # Retreating
                    # Arrow pointing from initial position away from projection point
                    ax.arrow(p1_positions[0][0], p1_positions[0][1], 
                            -arrow_length * connect_direction[0], -arrow_length * connect_direction[1],
                            head_width=arrow_width, head_length=arrow_width*1.5, 
                            fc=p1_color, ec='black', linewidth=1, zorder=4)
                
                # Add value label near the projection line
                mid_x = (p1_positions[0][0] + p1_proj_point[0]) / 2
                mid_y = (p1_positions[0][1] + p1_proj_point[1]) / 2
                ax.text(mid_x, mid_y, f"{p1_approach:.2f}", 
                        color=p1_color, fontsize=10, fontweight='bold',
                        bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2'))
            
            # For P2
            p2_approach = p2_approach_values[i]
            if abs(p2_approach) > 0.1:  # Only draw if there's a significant approach value
                # Draw line with increased width
                ax.plot([p2_positions[0][0], p2_proj_point[0]], 
                        [p2_positions[0][1], p2_proj_point[1]], 
                        '--', color=p2_color, linewidth=2.5)
                
                # Add arrow to show direction - make arrow larger
                arrow_length = min(abs(p2_approach) * 0.2, 0.6)  # Scale arrow size
                arrow_width = 0.15
                if p2_approach > 0:  # Approaching
                    # Arrow pointing from initial position toward P1
                    ax.arrow(p2_positions[0][0], p2_positions[0][1], 
                            arrow_length * p2_direction[0], arrow_length * p2_direction[1],
                            head_width=arrow_width, head_length=arrow_width*1.5, 
                            fc=p2_color, ec='black', linewidth=1, zorder=4)
                else:  # Retreating
                    # Arrow pointing from initial position away from P1
                    ax.arrow(p2_positions[0][0], p2_positions[0][1], 
                            -arrow_length * p2_direction[0], -arrow_length * p2_direction[1],
                            head_width=arrow_width, head_length=arrow_width*1.5, 
                            fc=p2_color, ec='black', linewidth=1, zorder=4)
                
                # Add value label near the projection line
                mid_x = (p2_positions[0][0] + p2_proj_point[0]) / 2
                mid_y = (p2_positions[0][1] + p2_proj_point[1]) / 2
                ax.text(mid_x, mid_y, f"{p2_approach:.2f}", 
                        color=p2_color, fontsize=10, fontweight='bold',
                        bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2'))
            
            # Add small markers at projection points
            ax.scatter(p1_proj_point[0], p1_proj_point[1], 
                      color=p1_color, s=70, marker='x')
            ax.scatter(p2_proj_point[0], p2_proj_point[1], 
                      color=p2_color, s=70, marker='x')
            
            # Draw horizontal dotted line at initial y-coordinate to emphasize horizontal start
            ax.axhline(y=p1_initial[1], color='gray', linestyle=':', alpha=0.5)
            
        # Add legend
        legend_elements = [
            patches.Patch(facecolor=p1_color, edgecolor='black', label='Person 1'),
            patches.Patch(facecolor=p2_color, edgecolor='black', label='Person 2'),
            patches.Patch(facecolor=vector_color, edgecolor='black', label='Connection Vector'),
            mlines.Line2D([], [], color='gray', linestyle=':', label='Initial Horizontal Line'),
            mlines.Line2D([], [], color=p1_color, linestyle='--', linewidth=2, label='Projection Length = Approach Value')
        ]
        ax.legend(handles=legend_elements, loc='upper right', fontsize=10)
        
        # Add a frame counter
        ax.text(0.95, 0.05, f"Frame: {i}/{num_frames-1}", transform=ax.transAxes,
               horizontalalignment='right', verticalalignment='bottom', fontsize=10,
               bbox=dict(boxstyle='round', facecolor='white', alpha=0.5))
        
        # Add description text
        description = (
            "Proximity Calculation Visualization\n"
            "- Large dots: Current positions\n"
            "- Small dots: Initial positions (horizontal alignment)\n"
            "- Green line: Current connecting vector\n"
            "- Dashed lines: Projection of movement (length = approach value)\n"
            "- Arrows: Direction of approach/retreat\n"
            "- Positive values: Movement toward other person\n"
            "- Negative values: Movement away from other person\n"
            "- X markers: Projected positions"
        )
        ax.text(0.05, 0.05, description, transform=ax.transAxes,
               verticalalignment='bottom', fontsize=9,
               bbox=dict(boxstyle='round', facecolor='white', alpha=0.7))
        
        return []

    # Create the animation with explicit FPS and DPI
    ani = FuncAnimation(fig, animate, frames=num_frames, init_func=init, 
                        blit=True, interval=100)
    
    # Save the animation as a GIF with explicit DPI
    ani.save('images/proximity_visualization/proximity_calculation_extreme.gif', writer='pillow', fps=10, dpi=100)
    plt.close(fig)

# Run the animation
try:
    print("Creating animation...")
    create_detailed_animation()
    print("GIF created successfully in the 'images/proximity_visualization' folder.")
except Exception as e:
    print(f"Error creating animation: {e}")
    
    # Alternative approach if the above fails - create static frames
    print("Trying alternative approach with static frames...")
    try:
        # Create a static image sequence instead
        os.makedirs("images/proximity_visualization/frames", exist_ok=True)
        
        # Create 10 static frames instead of full animation
        sample_frames = np.linspace(0, num_frames-1, 10, dtype=int)
        
        for idx, i in enumerate(sample_frames):
            fig, ax = plt.subplots(figsize=(10, 8), dpi=100)
            
            # Set up the axes
            ax.set_xlim(0, 10)
            ax.set_ylim(0, 10)
            ax.set_title(f'Proximity Calculation (Frame {i})', fontsize=14)
            
            # Get current positions
            p1_pos = p1_positions[i]
            p2_pos = p2_positions[i]
            
            # Draw the connection vector
            ax.plot([p1_pos[0], p2_pos[0]], [p1_pos[1], p2_pos[1]], 
                    color=vector_color, linewidth=2)
            
            # Draw person markers
            ax.scatter(p1_pos[0], p1_pos[1], s=150, color=p1_color, label='Person 1')
            ax.scatter(p2_pos[0], p2_pos[1], s=150, color=p2_color, label='Person 2')
            
            # Draw initial positions
            ax.scatter(p1_positions[0][0], p1_positions[0][1], s=50, color=p1_color, alpha=0.5)
            ax.scatter(p2_positions[0][0], p2_positions[0][1], s=50, color=p2_color, alpha=0.5)
            
            # Draw horizontal dotted line at initial y-coordinate
            ax.axhline(y=p1_initial[1], color='gray', linestyle=':', alpha=0.5)
            
            # Show approach values
            ax.text(0.5, 0.95, f"Person 1 approach: {p1_approach_values[i]:.2f}", 
                    transform=ax.transAxes, ha='center', va='top',
                    bbox=dict(boxstyle='round', facecolor=p1_color, alpha=0.3))
            
            ax.text(0.5, 0.9, f"Person 2 approach: {p2_approach_values[i]:.2f}", 
                    transform=ax.transAxes, ha='center', va='top',
                    bbox=dict(boxstyle='round', facecolor=p2_color, alpha=0.3))
            
            ax.legend()
            
            # Save the frame
            plt.tight_layout()
            plt.savefig(f'images/proximity_visualization/frames/frame_{idx:02d}.png')
            plt.close(fig)
            
        print("Static frames created successfully in 'images/proximity_visualization/frames' folder.")
    except Exception as e:
        print(f"Alternative approach also failed: {e}")
```

:::

![Proximity Calculation Visualization](./images/proximity_visualization/proximity_calculation.gif)

We further check for any remaining missing values after processing and confirms time continuity to detect any frame drops or temporal misalignments. 

The output of this script is a timeseries matrix for each video, which contains the time in seconds and the calculated variables. The timeseries matrices are saved in the `./dataoutput_STEP1_2_timeseries/` folder. The code below is the script `STEP2_process_timeseries.py` that you can run to process the raw pose data.

::: {.callout-note .callout-installation collapse="true"}
## Pipeline code step 1.2 for processing to timeseries data ðŸš©

```{python, eval=FALSE, code_folding="hide"}
import pandas as pd
import numpy as np
import glob
import os
import cv2
import math
from bisect import bisect_left
from scipy.signal import savgol_filter

# Path Definitions
INPUT_LAYER1_PATH = '../dataoutput_STEP1_1_rawposedata/'  # Input directory containing tracked keypoint data from STEP1
VIDEO_PATH = "../data_raw/"                        # Raw video files directory
OUTPUT_PATH = '../dataoutput_STEP1_2_timeseries/'     # Output directory for processed timeseries data

# Variable Explanations:
# =====================================================
# Positional Variables:
# - x, y: 2D coordinates in the image frame
# - x_min, x_max, y_min, y_max: Bounding box coordinates for a person
# - centroid_x, centroid_y: Center point of a person's bounding box
# - com_x, com_y: Center of mass for a person (average of upper body keypoint positions)
# - shoulder_midpoint_*: Midpoint between left and right shoulders for each person (p1, p2)
# - wrist_left_*, wrist_right_*: Positions of left and right wrists for each person (p1, p2)
#
# Distance Variables:
# - distance: Euclidean distance between the centroids of both people
# - distance_com: Euclidean distance between centers of mass of both people
# - distance_shoulder_midpoint: Distance between shoulder midpoints of both people
# - *_speed: Euclidean speed of different body parts (calculated from position differences)
# - *_speed_smooth: Smoothed version of speed using Savitzky-Golay filter
#
# Movement Analysis:
# - p1_com_approach_pos, p2_com_approach_pos: Position of each person's COM relative to their
#   initial position, projected onto the connecting line between people. Positive values 
#   indicate movement toward the other person.
#
# Tracking Status:
# - both_tracked: Boolean indicating if both people are tracked in the current frame
# - single_tracked: Boolean indicating if only one person is tracked in the current frame
# =====================================================


def frame_to_time(ts, fps):
    """Convert frame numbers to time in seconds"""
    ts["time"] = [row[1]["frame"] / fps for row in ts.iterrows()]
    return ts


def take_closest(myList, myNumber):
    """
    Assumes myList is sorted. Returns closest value to myNumber.
    If two numbers are equally close, return the smallest number.
    """
    pos = bisect_left(myList, myNumber)
    if pos == 0:
        return myList[0]
    if pos == len(myList):
        return myList[-1]
    before = myList[pos - 1]
    after = myList[pos]
    if after - myNumber < myNumber - before:
        return after
    else:
        return before


def assign_left_right(ts):
    """Assign person IDs (0 or 1) based on x-position in the frame"""
    min_x = np.min([val for val in ts['x'] if val != 0])
    max_x = np.max([val for val in ts['x'] if val != 0])
    for index, row in ts.iterrows():
        if row['x'] == 0:  # Skip untracked points
            continue
        if take_closest([min_x, max_x], row['x']) == min_x:
            ts.loc[index, 'person'] = 0  # Left person
        else:
            ts.loc[index, 'person'] = 1  # Right person
    return ts


def process_time_data(ts):
    """Calculate statistics for tracked people using upper body keypoints"""
    ts_upper = ts[ts['keypoint'] < 11]  # Filter to upper body keypoints only
    
    # Calculate stats per person and time
    stats = ts_upper.groupby(['time', 'person']).agg({
        'x': ['min', 'max', 'mean'],
        'y': ['min', 'max', 'mean']
    }).reset_index()
    
    stats.columns = ['time', 'person', 'x_min', 'x_max', 'com_x', 'y_min', 'y_max', 'com_y']
    stats['centroid_x'] = (stats['x_min'] + stats['x_max']) / 2
    stats['centroid_y'] = (stats['y_min'] + stats['y_max']) / 2
    
    return stats


def process_people_data_vectorized(ts, stats):
    """
    Process keypoint data to calculate relative positions and distances.
    Preserves time alignment and handles COM/centroid assignment.
    """
    # Get all unique times from the original data
    all_times = sorted(ts['time'].unique())
    result = pd.DataFrame(index=all_times)
    result.index.name = 'time'
    
    # Process shoulders first (keypoints 5 and 6)
    shoulders = ts[ts['keypoint'].isin([5, 6])].groupby(['time', 'person']).agg({
        'x': 'mean',
        'y': 'mean'
    }).reset_index()
    
    # Process wrists (keypoints 7 and 8)
    wrists = ts[ts['keypoint'].isin([7, 8])].pivot_table(
        index=['time', 'person'],
        columns='keypoint',
        values=['x', 'y']
    ).reset_index()
    wrists.columns = ['time', 'person', 'left_x', 'right_x', 'left_y', 'right_y']
    
    # Get person-specific data
    p0_stats = stats[stats['person'] == 0].set_index('time')
    p1_stats = stats[stats['person'] == 1].set_index('time')
    
    # Process distances where both people exist
    common_times = sorted(set(p0_stats.index) & set(p1_stats.index))
    if common_times:  # Only calculate if we have common times
        result.loc[common_times, 'distance'] = np.sqrt(
            (p0_stats.loc[common_times, 'centroid_x'] - p1_stats.loc[common_times, 'centroid_x'])**2 + 
            (p0_stats.loc[common_times, 'centroid_y'] - p1_stats.loc[common_times, 'centroid_y'])**2
        )
        result.loc[common_times, 'distance_com'] = np.sqrt(
            (p0_stats.loc[common_times, 'com_x'] - p1_stats.loc[common_times, 'com_x'])**2 + 
            (p0_stats.loc[common_times, 'com_y'] - p1_stats.loc[common_times, 'com_y'])**2
        )
    
    # Process shoulders per person
    for person, prefix in [(0, 'p1'), (1, 'p2')]:
        s_person = shoulders[shoulders['person'] == person].set_index('time')
        result[f'shoulder_midpoint_{prefix}_x'] = s_person['x']
        result[f'shoulder_midpoint_{prefix}_y'] = s_person['y']
    
    # Calculate shoulder midpoint distance where possible
    shoulder_cols = ['shoulder_midpoint_p1_x', 'shoulder_midpoint_p2_x',
                    'shoulder_midpoint_p1_y', 'shoulder_midpoint_p2_y']
    shoulder_mask = result[shoulder_cols].notna().all(axis=1)
    if shoulder_mask.any():
        result.loc[shoulder_mask, 'distance_shoulder_midpoint'] = np.sqrt(
            (result.loc[shoulder_mask, 'shoulder_midpoint_p1_x'] - result.loc[shoulder_mask, 'shoulder_midpoint_p2_x'])**2 + 
            (result.loc[shoulder_mask, 'shoulder_midpoint_p1_y'] - result.loc[shoulder_mask, 'shoulder_midpoint_p2_y'])**2
        )
    
    # Process wrists per person and add COM/centroid
    for person, prefix in [(0, 'p1'), (1, 'p2')]:
        w_person = wrists[wrists['person'] == person].set_index('time')
        result[f'wrist_left_{prefix}_x'] = w_person['left_x']
        result[f'wrist_left_{prefix}_y'] = w_person['left_y']
        result[f'wrist_right_{prefix}_x'] = w_person['right_x']
        result[f'wrist_right_{prefix}_y'] = w_person['right_y']
        
        # Use the correct stats object based on person ID
        person_stats = p0_stats if person == 0 else p1_stats
            
        # Add com and centroid with the correct stats object
        result[f'com_{prefix}_x'] = person_stats['com_x']
        result[f'com_{prefix}_y'] = person_stats['com_y']
        result[f'centroid_{prefix}_x'] = person_stats['centroid_x']
        result[f'centroid_{prefix}_y'] = person_stats['centroid_y']

    # Add tracking quality indicators using original data
    person_counts = ts.groupby('time')['person'].nunique()
    result['both_tracked'] = result.index.map(lambda x: person_counts.get(x, 0) == 2)
    result['single_tracked'] = result.index.map(lambda x: person_counts.get(x, 0) == 1)
        
    return result


def calculate_proximity_approach(timeseries_data):
    """
    Calculate each person's position relative to their initial position,
    projecting movement onto the current connecting line between people.
    
    This handles rotation and changing spatial relationships between people.
    Positive values indicate movement toward the other person from initial position.
    
    The reference positions are only established when both people are detected.
    """
    # Create columns for our measurements
    if 'p1_com_approach_pos' not in timeseries_data.columns:
        timeseries_data['p1_com_approach_pos'] = np.nan
    if 'p2_com_approach_pos' not in timeseries_data.columns:
        timeseries_data['p2_com_approach_pos'] = np.nan
    
    # Sort data by time
    sorted_data = timeseries_data.sort_values('time')
    
    # Find first valid frame to establish reference positions
    reference_p1_pos = None
    reference_p2_pos = None
    reference_distance = None
    
    # First pass: find reference frame where both people are detected
    for idx, row in sorted_data.iterrows():
        # Skip rows with NaN values or where both_tracked is False
        if (np.isnan(row['com_p1_x']) or np.isnan(row['com_p1_y']) or 
            np.isnan(row['com_p2_x']) or np.isnan(row['com_p2_y']) or
            ('both_tracked' in row.index and row['both_tracked'] == False)):
            continue
            
        # Get positions for this frame
        p1_pos = np.array([row['com_p1_x'], row['com_p1_y']])
        p2_pos = np.array([row['com_p2_x'], row['com_p2_y']])
        
        # Calculate connecting vector
        connect_vector = p2_pos - p1_pos
        distance = np.linalg.norm(connect_vector)
        
        if distance > 0:
            # We found a valid reference frame
            reference_p1_pos = p1_pos.copy()
            reference_p2_pos = p2_pos.copy()
            reference_distance = distance
            print(f"Reference frame established at time={row['time']}")
            print(f"  Reference p1_pos: {reference_p1_pos}")
            print(f"  Reference p2_pos: {reference_p2_pos}")
            print(f"  Reference distance: {reference_distance}")
            break
    
    if reference_p1_pos is None:
        print("ERROR: Could not establish a reference frame. No valid frames found with both people detected.")
        return timeseries_data
    
    # Second pass: calculate projected positions for all frames
    for idx, row in sorted_data.iterrows():
        # Skip rows with NaN values
        if (np.isnan(row['com_p1_x']) or np.isnan(row['com_p1_y']) or 
            np.isnan(row['com_p2_x']) or np.isnan(row['com_p2_y'])):
            continue
            
        # Get current positions
        p1_pos = np.array([row['com_p1_x'], row['com_p1_y']])
        p2_pos = np.array([row['com_p2_x'], row['com_p2_y']])
        
        # Calculate current connecting vector and direction
        current_connect = p2_pos - p1_pos
        current_distance = np.linalg.norm(current_connect)
        
        # Skip frames where people are at the same position
        if current_distance == 0:
            continue
            
        current_direction = current_connect / current_distance
        
        # Calculate vector from reference position to current position
        p1_vector = p1_pos - reference_p1_pos
        p2_vector = p2_pos - reference_p2_pos
        
        # Project these vectors onto the current connecting line
        # Positive values mean moving toward the other person
        p1_projection = np.dot(p1_vector, current_direction)
        p2_projection = -np.dot(p2_vector, -current_direction)
        
        # Store values
        timeseries_data.loc[idx, 'p1_com_approach_pos'] = p1_projection
        timeseries_data.loc[idx, 'p2_com_approach_pos'] = p2_projection
    
    # Verify results
    filled_p1 = timeseries_data['p1_com_approach_pos'].notna().sum()
    filled_p2 = timeseries_data['p2_com_approach_pos'].notna().sum()
    
    print(f"Frames with filled values - p1: {filled_p1}, p2: {filled_p2}")
    
    return timeseries_data


def main():
    """Main processing function"""
    # Create output directory if it doesn't exist
    os.makedirs(OUTPUT_PATH, exist_ok=True)
    
    # Get all CSV files from layer 1 processing
    layer1_files = glob.glob(INPUT_LAYER1_PATH + '*.csv')
    
    # Process each file
    for file_path in layer1_files:
        # Extract video name from file path
        vid_name = os.path.basename(file_path).split('_keypoints_data_layer1.csv')[0]
        print(f"Processing video: {vid_name}")
        
        # Check if already processed
        output_file = f"{OUTPUT_PATH}/{vid_name}_processed_data_layer2.csv"
        if os.path.exists(output_file):
            print("Already processed, skipping...")
            continue
            
        # Determine video format and get FPS
        video_path = None
        for ext in ['.avi', '.mp4', '.mov']:
            if os.path.exists(f"{VIDEO_PATH}/{vid_name}{ext}"):
                video_path = f"{VIDEO_PATH}/{vid_name}{ext}"
                break
                
        if not video_path:
            print(f"Video file not found for {vid_name}")
            continue
            
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        cap.release()
        print(f"Working on: {vid_name} with fps = {fps}")
        
        # Load and preprocess data
        ts = pd.read_csv(file_path)
        ts = frame_to_time(ts, fps=fps)
        ts = assign_left_right(ts)
        
        # Calculate statistics
        stats = process_time_data(ts)
        
        # Process tracked data
        processed_data = process_people_data_vectorized(ts, stats)
        
        # Create final data frame
        bb_data = pd.merge(
            stats, 
            processed_data.reset_index(), 
            on='time', 
            how='outer'
        )
        
        # Extract time series data (using person 0 as reference)
        timeseries_data = bb_data[bb_data['person'] == 0].copy()  # Make a copy to avoid SettingWithCopyWarning
        
        # Remove unnecessary columns
        timeseries_data = timeseries_data.drop(columns='person')
        
        # Calculate desired time step based on FPS
        desired_time_step = 1/fps
        
        # Interpolate missing values
        nan_cols = timeseries_data.columns[timeseries_data.isna().any()].tolist()
        timeseries_data[nan_cols] = timeseries_data[nan_cols].interpolate()
        
        # Smooth distance with Savitzky-Golay filter
        timeseries_data['distance_smooth'] = savgol_filter(timeseries_data['distance'].values, 11, 3)
        
        # Calculate and smooth wrist speeds
        for wrist in ['wrist_left_p1', 'wrist_right_p1', 'wrist_left_p2', 'wrist_right_p2']:
            # Calculate speed as Euclidean distance between consecutive positions
            timeseries_data[f'{wrist}_speed'] = np.sqrt(
                timeseries_data[f'{wrist}_x'].diff()**2 + timeseries_data[f'{wrist}_y'].diff()**2
            )
            
            # Apply Savitzky-Golay filter for smoothing
            timeseries_data[f'{wrist}_speed_smooth'] = savgol_filter(
                timeseries_data[f'{wrist}_speed'].values, 11, 3
            )
        
        # Fill NaN values before calculating proximity approach
        timeseries_data = timeseries_data.fillna(method='ffill')
        
        # Calculate proximity approach
        timeseries_data = calculate_proximity_approach(timeseries_data)
        
        # Fill any remaining NaN values
        timeseries_data = timeseries_data.fillna(method='ffill')
        
        # Smooth proximity approach values
        timeseries_data['p1_com_approach_pos'] = savgol_filter(timeseries_data['p1_com_approach_pos'].values, 11, 3)
        timeseries_data['p2_com_approach_pos'] = savgol_filter(timeseries_data['p2_com_approach_pos'].values, 11, 3)
        
        # Check for any remaining NaN values
        print("Checking for NaN values...")
        nan_counts = timeseries_data.isna().sum()
        if nan_counts.sum() > 0:
            print("Found NaN values after processing:")
            print(nan_counts)
        else:
            print("No NaN values found.")
        
        # Save processed data
        timeseries_data.to_csv(output_file, index=False)
        
        # Check for time continuity
        print("Checking time continuity...")
        time_diffs = np.array([round(val, 2) for val in np.diff(timeseries_data['time'])])
        if not np.all(time_diffs == desired_time_step):
            print(f"Found gaps at times: {np.where(time_diffs != desired_time_step)[0]}")
        else:
            print("No missing time points.")

if __name__ == "__main__":
    main()
```

:::

To provide an example of the data we plot here a timeseries of the resultant data.


::: {.callout-note .callout-installation collapse="true"}
## Example output timeseries: ðŸ“Š

```{python examplecsvfilelayer12}
import pandas as pd
import glob as glob
import os

folderstep12output = "./dataoutput_STEP1_2_timeseries/"
# Load the CSV file
csv_file = glob.glob(os.path.join(folderstep12output + "*.csv"))[0]
df = pd.read_csv(csv_file)

# Display the first few rows of the DataFrame
print(df.head())
```

:::

## Step 1.3: Animation with original videodata

We advise users to double check the quality of the tracking data and the derived variables. The animation below is generated using the code `STEP3_animation.py` which couples video with quantiative data, allowing for qualitatively checking the timeseries real time against what is happening in the video. The user can select variables need to be plotted in the video plus time series animation. The code below provides the animation. Please check the DIY section for how to set up the motion tracking and run the code.

::: {.callout-note .callout-installation collapse="true"}
## Code chunk: Animation code for coupling video with quantitative data ðŸš© 

```{python, eval=FALSE, code_folding="hide"}
import pandas as pd
import numpy as np
import glob
import os
import cv2
import math
from bisect import bisect_left
from scipy.signal import savgol_filter
import matplotlib.pyplot as plt
import seaborn as sns
import tqdm
import tempfile
from moviepy import VideoFileClip

# Path Definitions
INPUT_LAYER1_PATH = '../dataoutput_STEP1_2_timeseries/'  # Input directory containing tracked keypoint data from STEP1
VIDEO_PATH = "../dataoutput_STEP1_1_rawposedata/"                        # Raw video files directory
OUTPUT_PATH = '../dataoutput_STEP1_3_animations/'     # Output directory for processed timeseries data
targetvideo = "*sample_annotated_layer1_c150_miss95.mp4" # note that sample video must be set to True to process only a sample video
SAMPLE_VIDEO = True  # Set to True to process only a sample video, False to process all videos
# Create output directory if it doesn't exist
os.makedirs(OUTPUT_PATH, exist_ok=True)

# animate only sample video?

if SAMPLE_VIDEO:
    # For sample video, use a specific video file
    allvidsnew =  glob.glob(VIDEO_PATH + "*" + targetvideo)
else:
    allvidsnew = glob.glob(VIDEO_PATH + "*.mp4")

print(f"Found {len(allvidsnew)} videos to process")

# what variables to animate with video
animate_variables = {
    'x_min': False,
    'x_max': False,
    'com_x': False,
    'y_min': False,
    'y_max': False,
    'com_y': False,
    'centroid_x': False,
    'centroid_y': False,
    'distance': False,
    'distance_com': False,
    'shoulder_midpoint_p1_x': False,
    'shoulder_midpoint_p1_y': False,
    'shoulder_midpoint_p2_x': False,
    'shoulder_midpoint_p2_y': False,
    'distance_shoulder_midpoint': True,
    'wrist_left_p1_x': False,
    'wrist_left_p1_y': False,
    'wrist_right_p1_x': False,
    'wrist_right_p1_y': False,
    'com_p1_x': False,
    'com_p1_y': False,
    'centroid_p1_x': False,
    'centroid_p1_y': False,
    'wrist_left_p2_x': False,
    'wrist_left_p2_y': False,
    'wrist_right_p2_x': False,
    'wrist_right_p2_y': False,
    'com_p2_x': False,
    'com_p2_y': False,
    'centroid_p2_x': False,
    'centroid_p2_y': False,
    'distance_smooth': True,
    'wrist_left_p1_speed': False,
    'wrist_left_p1_speed_smooth': True,
    'wrist_right_p1_speed': False,
    'wrist_right_p1_speed_smooth': True,
    'wrist_left_p2_speed': False,
    'wrist_left_p2_speed_smooth': True,
    'wrist_right_p2_speed': False,
    'wrist_right_p2_speed_smooth': True,
    'p1_com_approach_pos': True,
    'p2_com_approach_pos': True
}

# plotting functions
def plot_timeseries(timeseries_data, current_time=None, figsize=(15, 8)):
    """
    Visualizing of the processed 1_2 motion variable data together with the original video.
    Groups variables by modality with consistent coloring for p1/p2 and left/right.
    
    Parameters:
    -----------
    timeseries_data : pandas.DataFrame
         DataFrame containing all the motion analysis columns
    current_time : float, optional
         current time in seconds to mark with vertical line
    figsize : tuple, optional
         Figure size in inches (width, height)
    """
    # Filter data for only variables marked as True in animate_variables
    active_vars = [var for var, active in animate_variables.items() if active and var in timeseries_data.columns]
    
    if not active_vars:
        print("No variables selected for animation!")
        return None
    
    # Define consistent color scheme
    # Colors for p1/p2 (blue family for p1, red family for p2)
    p1_colors = {'left': '#1f77b4', 'right': '#aec7e8'}  # Dark blue, light blue
    p2_colors = {'left': '#d62728', 'right': '#ff9896'}  # Dark red, light red
    other_colors = ['#ff7f0e', '#2ca02c', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']
    
    # Group variables by modality
    distance_vars = [var for var in active_vars if 'distance' in var]
    position_vars = [var for var in active_vars if any(x in var for x in ['_x', '_y', 'approach_pos']) and 'distance' not in var]
    speed_vars = [var for var in active_vars if 'speed' in var]
    
    # Calculate number of subplots needed
    n_plots = sum([len(distance_vars) > 0, len(position_vars) > 0, len(speed_vars) > 0])
    
    if n_plots == 0:
        return None
    
    # Create figure with subplots
    fig, axes = plt.subplots(n_plots, 1, figsize=figsize, squeeze=False)
    axes = axes.flatten()
    
    plot_idx = 0
    
    # Helper function to get consistent color and style
    def get_style(var_name):
        # Determine person (p1/p2)
        if '_p1_' in var_name:
            person = 'p1'
        elif '_p2_' in var_name:
            person = 'p2'
        else:
            person = 'other'
        
        # Determine side (left/right)
        if 'left' in var_name:
            side = 'left'
        elif 'right' in var_name:
            side = 'right'
        else:
            side = 'other'
        
        # Get color
        if person == 'p1':
            color = p1_colors.get(side, p1_colors['left'])
        elif person == 'p2':
            color = p2_colors.get(side, p2_colors['left'])
        else:
            color = other_colors[0]  # Use first color for non-person variables
        
        # Get linestyle (solid for left, dashed for right)
        if side == 'right':
            linestyle = '--'
        else:
            linestyle = '-'
        
        # Get alpha (lower for raw data, full for smoothed)
        if 'smooth' in var_name:
            alpha = 1.0
        elif any(x in var_name for x in ['speed', 'velocity']) and 'smooth' not in var_name:
            alpha = 0.3
        else:
            alpha = 1.0
        
        return color, linestyle, alpha
    
    # 1. Distance Plots
    if distance_vars:
        ax = axes[plot_idx]
        color_idx = 0
        for var in distance_vars:
            if any(x in var for x in ['_p1_', '_p2_']):
                color, linestyle, alpha = get_style(var)
            else:
                color = other_colors[color_idx % len(other_colors)]
                linestyle = '-'
                alpha = 1.0
                color_idx += 1
            
            ax.plot(timeseries_data['time'], timeseries_data[var], 
                   label=var.replace('_', ' '), linewidth=2, 
                   color=color, linestyle=linestyle, alpha=alpha)
        
        ax.set_title('Distances Over Time')
        ax.set_ylabel('Distance')
        ax.grid(True, alpha=0.3)
        ax.legend()
        if 'distance_shoulder_midpoint' in distance_vars or 'distance' in distance_vars:
            ax.invert_yaxis()
        plot_idx += 1
    
    # 2. Position Plots (group p1/p2 and left/right together)
    if position_vars:
        ax = axes[plot_idx]
        
        for var in position_vars:
            color, linestyle, alpha = get_style(var)
            ax.plot(timeseries_data['time'], timeseries_data[var], 
                   label=var.replace('_', ' '), linewidth=2, 
                   color=color, linestyle=linestyle, alpha=alpha)
        
        ax.set_title('Positions Over Time')
        ax.set_ylabel('Position')
        ax.grid(True, alpha=0.3)
        ax.legend()
        plot_idx += 1
    
    # 3. Speed Plots (group p1/p2 and left/right together)
    if speed_vars:
        ax = axes[plot_idx]
        
        for var in speed_vars:
            color, linestyle, alpha = get_style(var)
            ax.plot(timeseries_data['time'], timeseries_data[var], 
                   label=var.replace('_', ' '), linewidth=2, 
                   color=color, linestyle=linestyle, alpha=alpha)
        
        ax.set_title('Speeds Over Time')
        ax.set_ylabel('Speed')
        ax.grid(True, alpha=0.3)
        ax.legend()
        plot_idx += 1
    
    # Add vertical line for current time if specified
    if current_time is not None:
        for ax in axes[:plot_idx]:
            ax.axvline(x=current_time, color='red', linestyle='--', alpha=0.7, linewidth=2)
    
    # Set common x-label
    axes[plot_idx-1].set_xlabel('Time (seconds)')
    
    # Adjust layout
    plt.tight_layout()
    
    return fig

# Create animation for each video
for vids in allvidsnew:
    vidname = os.path.basename(vids)
    # remove substring "_annotated_layer1"
    lab = "_annotated_layer1"
    vidname = vidname.replace(lab, "")     
    vidname = vidname[:-4]
    # also remove substring "_c150_miss95"
    vidname = vidname.replace("_c150_miss95", "")
    
    print(f"\nProcessing video: {vidname}")
    
    
    # Check if CSV file exists
    csv_path = os.path.join(INPUT_LAYER1_PATH, f'{vidname}_processed_data_layer2.csv')
    if not os.path.exists(csv_path):
        print(f"CSV file not found: {csv_path}")
        continue
        
    # Load the CSV file
    timeseries_data = pd.read_csv(csv_path)
    
    # Output paths
    temp_output = os.path.join(OUTPUT_PATH, f'{vidname}_distance_layer2_temp.mp4')
    final_output = os.path.join(OUTPUT_PATH, f'{vidname}_distance_layer2.mp4')
    
    # if already exists, skip
    if os.path.exists(final_output):
        print("Already processed, skipping...")
        continue
    
    # load the video file in opencv
    cap = cv2.VideoCapture(vids)
    if not cap.isOpened():
        print(f"Error opening video: {vids}")
        continue
        
    # Get video properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    print(f"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames")
    
    # Calculate time step based on FPS
    time_step = 1.0 / fps
    print(f"Time step: {time_step:.4f} seconds per frame")
    
    # Define the output video writer
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(temp_output, fourcc, fps, (width, height))
    
    # Create temporary directory for plots
    with tempfile.TemporaryDirectory() as temp_dir:
        print("Creating animated video...")
        
        # loop over the frames with tqdm progress bar
        current_time = 0.0  # Start at time 0
        for frame_idx in tqdm.tqdm(range(total_frames), desc=f"Processing {vidname}"):
            # read the frame
            success, frame = cap.read()
            if not success:
                break
            
            # plot the timeseries with current time
            fig = plot_timeseries(timeseries_data, current_time)
            if fig is None:
                print("No plot generated, skipping frame")
                out.write(frame)
                current_time += time_step  # Increment by time step
                continue
            
            # save the plot to a temp file
            plot_path = os.path.join(temp_dir, f'plot_{frame_idx:06d}.png')
            fig.savefig(plot_path, dpi=100, bbox_inches='tight')
            plt.close(fig)  # Important: close figure to free memory
            
            # Read the plot image
            plot_img = cv2.imread(plot_path)
            if plot_img is None:
                print(f"Error reading plot image: {plot_path}")
                out.write(frame)
                current_time += time_step  # Increment by time step
                continue
            
            # Calculate overlay area (bottom third of the frame)
            slice_start = 2 * (height // 3)
            slice_height = height - slice_start
            
            # Resize plot to fit overlay area
            plot_img_resized = cv2.resize(plot_img, (width, slice_height))
            
            # Create overlay on the frame
            frame_copy = frame.copy()
            frame_copy[slice_start:slice_start + slice_height, :, :] = plot_img_resized
            
            # write the frame to the output video
            out.write(frame_copy)
            current_time += time_step  # Increment by time step (seconds)
        
        # Release everything
        cap.release()
        out.release()
        
    print(f"Temporary video saved, now re-encoding with MoviePy...")
    
    # Re-encode with MoviePy for better compatibility
    try:
        clip = VideoFileClip(temp_output)
        clip.write_videofile(final_output, codec='libx264', audio_codec='aac')
        clip.close()
        
        # Remove temporary file
        if os.path.exists(temp_output):
            os.remove(temp_output)
            
        print(f"Final output video saved as {final_output}")
        
    except Exception as e:
        print(f"Error during MoviePy re-encoding: {e}")
        print(f"Temporary video available at: {temp_output}")

print("\nAll videos processed!")
```

:::

![Animation of motion tracking data coupled with video](./images/com_distance_example.gif)

# Step 2: Summary feature extraction: Smoothness and other features
We have motivated in the main paper accompanying this notebook that smoothness can be a useful feature to understand the quality of interactions. And our pipeline focuses on the extraction of this specific feature, while we also show how easy it is to generate other features from the time series data that are processed after step 1.

We take two approaches to quantify smoothness of the movement data. The first approach is based on the spectral arc length metric, which has been shown to be a stable measure of smoothness for prolonged (quasi-cyclical) time series data [@balasubramanianAnalysisMovementSmoothness2015]. The approach is based on analyzing the frequency spectrum, specifically the changes in energy over the different frequencies, for the lower frequencies (< 10Hz; but this can be set by the user). Specifally it calculates the spectral arc length (distance of two points along a curve) from Fourier magnitude spectrum of the speed data (or in our case changes in the distance or proximity positions). This code is directly copied from the [original repository](https://github.com/siva82kb/smoothness) by the lead developer of the metric, Sivak Balasubramanian. We refer to the [@balasubramanianAnalysisMovementSmoothness2015] for further details and validation of this novel measure of smoothness. As can be seen, the higher values of the spectral arc length indicate smoother movements, while lower values indicate more jerky movements.

::: {.callout-note .callout-installation collapse="true"}
## Pipeline code step 2, function for calculating spectral arc length ðŸš© 
```{python, code_folding="hide"}
import numpy as np

# directly copied from the original repository https://github.com/siva82kb/smoothness/blob/master/python/smoothness.py
def spectral_arclength(movement, fs, padlevel=4, fc=10.0, amp_th=0.05):
    """
    Calcualtes the smoothness of the given speed profile using the modified spectral
    arc length metric.

    Parameters
    ----------
    movement : np.array
               The array containing the movement speed profile.
    fs       : float
               The sampling frequency of the data.
    padlevel : integer, optional
               Indicates the amount of zero padding to be done to the movement
               data for estimating the spectral arc length. [default = 4]
    fc       : float, optional
               The max. cut off frequency for calculating the spectral arc
               length metric. [default = 10.]
    amp_th   : float, optional
               The amplitude threshold to used for determing the cut off
               frequency upto which the spectral arc length is to be estimated.
               [default = 0.05]

    Returns
    -------
    sal      : float
               The spectral arc length estimate of the given movement's
               smoothness.
    (f, Mf)  : tuple of two np.arrays
               This is the frequency(f) and the magntiude spectrum(Mf) of the
               given movement data. This spectral is from 0. to fs/2.
    (f_sel, Mf_sel) : tuple of two np.arrays
                      This is the portion of the spectrum that is selected for
                      calculating the spectral arc length.

    Notes
    -----
    This is the modfieid spectral arc length metric, which has been tested only
    for discrete movements.
    It is suitable for movements that are a few seconds long, but for long
    movements it might be slow and results might not make sense (like any other
    smoothness metric).

    Examples
    --------
    >>> t = np.arange(-1, 1, 0.01)
    >>> move = np.exp(-5*pow(t, 2))
    >>> sal, _, _ = spectral_arclength(move, fs=100.)
    >>> '%.5f' % sal
    '-1.41403'

    """
    # Number of zeros to be padded.
    nfft = int(pow(2, np.ceil(np.log2(len(movement))) + padlevel))

    # Frequency
    f = np.arange(0, fs, fs/nfft)
    # Normalized magnitude spectrum
    Mf = abs(np.fft.fft(movement, nfft))
    Mf = Mf/max(Mf)

    # Indices to choose only the spectrum within the given cut off frequency Fc.
    # NOTE: This is a low pass filtering operation to get rid of high frequency
    # noise from affecting the next step (amplitude threshold based cut off for
    # arc length calculation).
    fc_inx = ((f <= fc)*1).nonzero()
    f_sel = f[fc_inx]
    Mf_sel = Mf[fc_inx]

    # Choose the amplitude threshold based cut off frequency.
    # Index of the last point on the magnitude spectrum that is greater than
    # or equal to the amplitude threshold.
    inx = ((Mf_sel >= amp_th)*1).nonzero()[0]
    fc_inx = range(inx[0], inx[-1]+1)
    f_sel = f_sel[fc_inx]
    Mf_sel = Mf_sel[fc_inx]

    # Calculate arc length
    new_sal = -sum(np.sqrt(pow(np.diff(f_sel)/(f_sel[-1] - f_sel[0]), 2) +
                           pow(np.diff(Mf_sel), 2)))
    return new_sal, (f, Mf), (f_sel, Mf_sel)
```

:::

The second approach is based on the log dimensionless squared jerk metric, which is the traditional measure of smoothness in biomechanics [@hoganSensitivitySmoothnessMeasures2009]. Below the function that calculates the dimensionless squared jerk metric from position data.  This measure is based on taking the third derivative with respect to time (jerk), squared and integrated over the movement duration, and then normalized by the amplitude of the movement. The log of this value is taken keep the values in a reasonable range. 

## Example smoothness results on simulated time series data

In the below example we show how the two smoothness metrics behave for three different time series data. The first time series is a smooth sinusoidal signal, the second is a noisy version of the first, and the third is a more noisy version of the second. The spectral arc length metric should be higher for the smooth signal, while the dimensionless squared jerk metric should be lower for the smooth signal (as it is less jerky).

::: {.callout-note .callout-installation collapse="true"}



::: {.callout-note .callout-installation collapse="true"}
## Pipeline code step 2, function for calculating log dimensionless jerk ðŸš© 

```{python, code_folding="hide"}
import numpy as np
from scipy import interpolate
from scipy.signal import savgol_filter
import pandas as pd
import scipy.integrate as integrate

# function to calculate the dimensionless squared jerk metric from position data
def dimensionless_squared_jerk_from_position(ts, time):
    """
    Calculate the dimensionless squared jerk metric from position data.
    
    Parameters:
    -----------
    ts : array_like
        Position data points, should be a 1D numpy array
    time : array_like
        Time points corresponding to the ts data, should be a 1D numpy array
    
    Returns:
    --------
    float
        Dimensionless squared jerk metric or NaN if calculation fails
    """
    dt = np.mean(np.diff(time))  # Calculate time step from the time array

    # Calculate speed (exactly as in original)
    speed = np.gradient(ts, dt)

    # Smooth savgol filter (maintaining original settings)
    speed = savgol_filter(speed, 11, 3)

    # Calculate acceleration (exactly as in original)
    acceleration = np.gradient(speed, dt)

    # Smooth (maintaining original settings)
    acceleration = savgol_filter(acceleration, 11, 3)
    
    # Calculate jerk (exactly as in original)
    jerk = np.gradient(acceleration, dt)
    
    # Smooth (maintaining original settings)
    jerk = savgol_filter(jerk, 11, 3)
    
    # Calculate movement duration (D)
    movement_duration = time[-1] - time[0]
    
    if movement_duration <= 0:
        print(f"Error: Movement duration must be positive. Got {movement_duration}")
        return np.nan
    
    # Calculate movement amplitude by integrating speed
    position = integrate.simpson(speed, x=time)
    movement_amplitude = abs(position)  # Use absolute value to ensure positive
    
    # Prevent division by zero
    epsilon = 1e-10
    if movement_amplitude < epsilon:
        print(f"Warning: Movement amplitude very small ({movement_amplitude}). Using epsilon.")
        movement_amplitude = epsilon
        
    # Calculate the squared jerk
    squared_jerk = jerk ** 2
    
    # Integrate the squared jerk
    integrated_squared_jerk = integrate.simpson(squared_jerk, x=time)
    
    # Ensure positive value for integral of squared jerk
    if integrated_squared_jerk < 0:
        print(f"Warning: Negative integral of squared jerk: {integrated_squared_jerk}. Using absolute value.")
        integrated_squared_jerk = abs(integrated_squared_jerk)
    
    # Calculate the dimensionless squared jerk
    dimensionless_jerk = integrated_squared_jerk * (movement_duration**5 / movement_amplitude**2)
    
    # Final sanity check
    if np.isnan(dimensionless_jerk) or np.isinf(dimensionless_jerk):
        print(f"Warning: Result is {dimensionless_jerk}. Details: ")
        print(f"  Movement duration: {movement_duration}")
        print(f"  Movement amplitude: {movement_amplitude}")
        print(f"  Integrated squared jerk: {integrated_squared_jerk}")
        return np.nan
    # log the result
    
    #print(f"Dimensionless squared jerk: {dimensionless_jerk}")
    # log the dimensionless jerk
    dimensionless_jerk = np.log(dimensionless_jerk)

    return dimensionless_jerk
```

:::


## Example code for checking behavior of the smoothness metrics ðŸ´
```{python, code_folding="hide"}
import matplotlib.pyplot as plt
import numpy as np

# Generate the time series data
ts1 = np.arange(5, 10, 0.01)
ts2 = np.arange(5, 10, 0.01) + np.random.normal(0, 0.1, len(ts1))
ts3 = np.arange(5, 10, 0.01) + np.random.normal(0, 0.5, len(ts1))

# Create sinusoidal time series
time_axis = np.arange(5, 10, 0.01)
ts1 = np.sin(ts1)
ts2 = np.sin(ts2)
ts3 = np.sin(ts3)

# Calculate metrics (assuming you have these functions defined)
spects1 = spectral_arclength(ts1, fs=1)
spects2 = spectral_arclength(ts2, fs=1)
spects3 = spectral_arclength(ts3, fs=1)

jerkts1 = dimensionless_squared_jerk_from_position(ts1, time_axis)
jerkts2 = dimensionless_squared_jerk_from_position(ts2, time_axis)
jerkts3 = dimensionless_squared_jerk_from_position(ts3, time_axis)

# Create figure with subplots - 1x3 layout
fig, axes = plt.subplots(1, 3, figsize=(18, 9))
fig.suptitle('Time Series Analysis: Spectral Arc Length and Dimensionless Squared Jerk', 
             fontsize=16, fontweight='bold', y=0.98)

# Colors for consistent styling
colors = ['#1f77b4', '#ff7f0e', '#d62728']  # Blue, Orange, Red

# Plot 1: Individual signals separated (left)
ax1 = axes[0]
offset = 3  # Vertical separation between signals
ax1.plot(time_axis, ts1 + 2*offset, color=colors[0], linewidth=2, label='Smooth Signal')
ax1.plot(time_axis, ts2 + offset, color=colors[1], linewidth=2, label='Low Noise Signal')
ax1.plot(time_axis, ts3, color=colors[2], linewidth=2, label='High Noise Signal')
ax1.set_title('Separated Time Series', fontweight='bold', fontsize=12)
ax1.set_xlabel('Time')
ax1.set_ylabel('Amplitude (Offset)')
ax1.legend(fontsize=10)
ax1.grid(True, alpha=0.3)

# Plot 2: Metrics comparison - Spectral Arc Length (middle)
ax2 = axes[1]
signals = ['Smooth\nSignal', 'Low Noise\nSignal', 'High Noise\nSignal']
spect_values = [spects1[0], spects2[0], spects3[0]]
bars1 = ax2.bar(signals, spect_values, color=colors, alpha=0.7, edgecolor='black', linewidth=1)
ax2.set_title('Spectral Arc Length Comparison', fontweight='bold', fontsize=12)
ax2.set_ylabel('Spectral Arc Length')
ax2.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for bar, value in zip(bars1, spect_values):
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)

# Plot 3: Metrics comparison - Dimensionless Squared Jerk (right)
ax3 = axes[2]
jerk_values = [jerkts1, jerkts2, jerkts3]
bars2 = ax3.bar(signals, jerk_values, color=colors, alpha=0.7, edgecolor='black', linewidth=1)
ax3.set_title('Dimensionless Squared Jerk Comparison', fontweight='bold', fontsize=12)
ax3.set_ylabel('Dimensionless Squared Jerk')
ax3.grid(True, alpha=0.3, axis='y')

# Add value labels on bars
for bar, value in zip(bars2, jerk_values):
    height = bar.get_height()
    ax3.text(bar.get_x() + bar.get_width()/2., height + max(jerk_values)*0.02,
             f'{value:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
# Adjust layout to prevent overlap
plt.tight_layout(rect=[0, 0.12, 1, 0.95])
#plt.show()
# save plot
plt.savefig('./images/smoothness_metrics_comparison.png', dpi=300, bbox_inches='tight')
```
:::

![Smoothness metrics comparison](./images/smoothness_metrics_comparison.png)

In the below pipeline code for step 2, we calculate smoothness for the distance between the two hands, as well as the distance to the center of mass (COM) and centroid of the two hands. We also calculate the smoothness of the wrist speed profiles, which are derived from the distance time series data. When SPARC is in the variable name, it refers to the spectral arc length metric, while smoothness will refer to the dimensionless squared jerk metric.

::: {.callout-note .callout-installation collapse="true"}
## Pipeline code step 2 complete procedure ðŸš©

```{python, eval=FALSE, code_folding="hide"}
import matplotlib.pyplot as plt
import seaborn as sns
import tempfile
from scipy import integrate
from scipy import interpolate
from scipy.signal import savgol_filter
import numpy as np
import pandas as pd
import os
import glob

# specifically we might be interested in computing the smoothness of the distance
inputfol = '../dataoutput_STEP1_2_timeseries/'
outputfol = '../dataoutput_STEP2_features/'
metadata = pd.read_csv('../meta/project_pointsibling_metadata_starttimes.csv', encoding='latin1')
constantwindowsize_sec = 100 # we want an equal portion of each timeseries to be analyzed

# function to calculate spectral arc length
def spectral_arclength(movement, fs, padlevel=4, fc=10.0, amp_th=0.05):
    """
    Calcualtes the smoothness of the given speed profile using the modified spectral
    arc length metric.

    Parameters
    ----------
    movement : np.array
               The array containing the movement speed profile.
    fs       : float
               The sampling frequency of the data.
    padlevel : integer, optional
               Indicates the amount of zero padding to be done to the movement
               data for estimating the spectral arc length. [default = 4]
    fc       : float, optional
               The max. cut off frequency for calculating the spectral arc
               length metric. [default = 10.]
    amp_th   : float, optional
               The amplitude threshold to used for determing the cut off
               frequency upto which the spectral arc length is to be estimated.
               [default = 0.05]

    Returns
    -------
    sal      : float
               The spectral arc length estimate of the given movement's
               smoothness.
    (f, Mf)  : tuple of two np.arrays
               This is the frequency(f) and the magntiude spectrum(Mf) of the
               given movement data. This spectral is from 0. to fs/2.
    (f_sel, Mf_sel) : tuple of two np.arrays
                      This is the portion of the spectrum that is selected for
                      calculating the spectral arc length.

    Notes
    -----
    This is the modfieid spectral arc length metric, which has been tested only
    for discrete movements.
    It is suitable for movements that are a few seconds long, but for long
    movements it might be slow and results might not make sense (like any other
    smoothness metric).

    Examples
    --------
    >>> t = np.arange(-1, 1, 0.01)
    >>> move = np.exp(-5*pow(t, 2))
    >>> sal, _, _ = spectral_arclength(move, fs=100.)
    >>> '%.5f' % sal
    '-1.41403'

    """
    # Number of zeros to be padded.
    nfft = int(pow(2, np.ceil(np.log2(len(movement))) + padlevel))

    # Frequency
    f = np.arange(0, fs, fs/nfft)
    # Normalized magnitude spectrum
    Mf = abs(np.fft.fft(movement, nfft))
    Mf = Mf/max(Mf)

    # Indices to choose only the spectrum within the given cut off frequency Fc.
    # NOTE: This is a low pass filtering operation to get rid of high frequency
    # noise from affecting the next step (amplitude threshold based cut off for
    # arc length calculation).
    fc_inx = ((f <= fc)*1).nonzero()
    f_sel = f[fc_inx]
    Mf_sel = Mf[fc_inx]

    # Choose the amplitude threshold based cut off frequency.
    # Index of the last point on the magnitude spectrum that is greater than
    # or equal to the amplitude threshold.
    inx = ((Mf_sel >= amp_th)*1).nonzero()[0]
    fc_inx = range(inx[0], inx[-1]+1)
    f_sel = f_sel[fc_inx]
    Mf_sel = Mf_sel[fc_inx]

    # Calculate arc length
    new_sal = -sum(np.sqrt(pow(np.diff(f_sel)/(f_sel[-1] - f_sel[0]), 2) +
                           pow(np.diff(Mf_sel), 2)))
    return new_sal, (f, Mf), (f_sel, Mf_sel)

# function to calculate the dimensionless squared jerk metric from position data
def dimensionless_squared_jerk_from_position(ts, time):
    """
    Calculate the dimensionless squared jerk metric from position data.
    
    Parameters:
    -----------
    ts : array_like
        Position data points, should be a 1D numpy array
    time : array_like
        Time points corresponding to the ts data, should be a 1D numpy array
    
    Returns:
    --------
    float
        Dimensionless squared jerk metric or NaN if calculation fails
    """
    try:
        # First check the raw inputs
        ts = np.array(ts, dtype=float)
        time = np.array(time, dtype=float)
        
        print(f"Original shape - ts: {ts.shape}, time: {time.shape}")
        print(f"NaN count before processing - ts: {np.isnan(ts).sum()}, time: {np.isnan(time).sum()}")
        
        # Input validation before preprocessing
        if len(ts) != len(time):
            print(f"Error: Arrays must have the same length. ts: {len(ts)}, time: {len(time)}")
            return np.nan
            
        if len(ts) < 11:  # Minimum length for savgol filter
            print(f"Error: Input arrays too short for savgol window size=11. Length: {len(ts)}")
            return np.nan
        
        # Check if time has NaNs - we need to fix time first
        if np.isnan(time).any():
            print("Warning: Time array contains NaNs, filling with linear sequence")
            valid_time_mask = ~np.isnan(time)
            if not valid_time_mask.any():
                print("Error: All time values are NaN")
                return np.nan
                
            # Create a proper time sequence
            time = np.linspace(np.nanmin(time), np.nanmax(time), len(time))
        
        # Check if input data is all NaN
        if np.isnan(ts).all():
            print("Error: All input values are NaN")
            return np.nan
        
        # Identify valid data points for interpolation
        valid_mask = ~np.isnan(ts)
        valid_indices = np.where(valid_mask)[0]
        
        if len(valid_indices) < 2:
            print(f"Error: Not enough valid data points for interpolation. Found {len(valid_indices)}")
            return np.nan
        
        # Handle the interpolation more carefully
        # 1. Use valid points to interpolate
        if np.isnan(ts).any():
            try:               
                # Create interpolator with valid points only
                valid_time = time[valid_mask]
                valid_data = ts[valid_mask]
                
                # Sort by time to ensure proper interpolation
                sort_idx = np.argsort(valid_time)
                valid_time = valid_time[sort_idx]
                valid_data = valid_data[sort_idx]
                
                # Create interpolation function
                f = interpolate.interp1d(
                    valid_time, valid_data,
                    bounds_error=False,
                    fill_value=(valid_data[0], valid_data[-1])  # Extrapolate with edge values
                )
                
                # Apply interpolation
                ts_interpolated = f(time)
                
                # Check if interpolation fixed all NaNs
                if np.isnan(ts_interpolated).any():
                    print(f"Warning: Interpolation failed to fix all NaNs. Remaining: {np.isnan(ts_interpolated).sum()}")
                    # Last resort: replace any remaining NaNs with nearest valid value
                    ts_interpolated = pd.Series(ts_interpolated).fillna(method='ffill').fillna(method='bfill').values
                
                ts = ts_interpolated
            except Exception as e:
                print(f"Error during interpolation: {str(e)}")
                return np.nan
        
        # Verify no NaNs remain after preprocessing
        if np.isnan(ts).any() or np.isnan(time).any():
            print("Error: Failed to remove all NaN values")
            return np.nan
            
        # Ensure time steps are uniform for derivative calculation
        uniform_time = np.linspace(time[0], time[-1], len(time))
        if not np.allclose(time, uniform_time):
            # If time is not uniform, resample the data
            print("Warning: Time steps not uniform, resampling data")
            # Use scipy's interpolation again to resample
            f = interpolate.interp1d(time, ts, bounds_error=False, fill_value='extrapolate')
            ts = f(uniform_time)
            time = uniform_time
        
        # Calculate the time step
        dt = np.diff(time)[0]
        
        if dt <= 0:
            print(f"Error: Time steps must be positive. Got dt={dt}")
            return np.nan
        
        # Print state after preprocessing
        print(f"Data after preprocessing - Length: {len(ts)}")
        print(f"NaN check after preprocessing - ts: {np.isnan(ts).any()}, time: {np.isnan(time).any()}")
        print(f"Range - ts: {np.min(ts)} to {np.max(ts)}, time: {np.min(time)} to {np.max(time)}")
        
        # Calculate speed (exactly as in original)
        speed = np.gradient(ts, dt)

        # Smooth savgol filter (maintaining original settings)
        speed = savgol_filter(speed, 11, 3)

        # Calculate acceleration (exactly as in original)
        acceleration = np.gradient(speed, dt)

        # Smooth (maintaining original settings)
        acceleration = savgol_filter(acceleration, 11, 3)
        
        # Calculate jerk (exactly as in original)
        jerk = np.gradient(acceleration, dt)
        
        # Smooth (maintaining original settings)
        jerk = savgol_filter(jerk, 11, 3)
        
        # Calculate movement duration (D)
        movement_duration = time[-1] - time[0]
        
        if movement_duration <= 0:
            print(f"Error: Movement duration must be positive. Got {movement_duration}")
            return np.nan
        
        # Calculate movement amplitude by integrating speed
        position = integrate.simpson(speed, x=time)
        movement_amplitude = abs(position)  # Use absolute value to ensure positive
        
        # Prevent division by zero
        epsilon = 1e-10
        if movement_amplitude < epsilon:
            print(f"Warning: Movement amplitude very small ({movement_amplitude}). Using epsilon.")
            movement_amplitude = epsilon
            
        # Calculate the squared jerk
        squared_jerk = jerk ** 2
        
        # Integrate the squared jerk
        integrated_squared_jerk = integrate.simpson(squared_jerk, x=time)
        
        # Ensure positive value for integral of squared jerk
        if integrated_squared_jerk < 0:
            print(f"Warning: Negative integral of squared jerk: {integrated_squared_jerk}. Using absolute value.")
            integrated_squared_jerk = abs(integrated_squared_jerk)
        
        # Calculate the dimensionless squared jerk
        dimensionless_jerk = integrated_squared_jerk * (movement_duration**5 / movement_amplitude**2)
        
        # Final sanity check
        if np.isnan(dimensionless_jerk) or np.isinf(dimensionless_jerk):
            print(f"Warning: Result is {dimensionless_jerk}. Details: ")
            print(f"  Movement duration: {movement_duration}")
            print(f"  Movement amplitude: {movement_amplitude}")
            print(f"  Integrated squared jerk: {integrated_squared_jerk}")
            return np.nan
        # log the result
        
        print(f"Dimensionless squared jerk: {dimensionless_jerk}")
        # log the dimensionless jerk
        dimensionless_jerk = np.log(dimensionless_jerk)

        return dimensionless_jerk
        
    except Exception as e:
        print(f"Error calculating dimensionless squared jerk: {str(e)}")
        return np.nan
    
# df for smoothness date
newdfcolumns = ['videoID','timeadjusted', 'originalsamples','adjustedsamples', 'start_time_analysiswindow', 'end_time_analysiswindow', 'perc_twopersonsdetected', 'average_com_movementp1', 'average_com_movementp2', 'smoothness_distancecom', 'SPARC_smoothness_distancecom', 'smoothness_distancecentroid', 'smoothness_xy_average_com_p1', 'smoothness_xy_average_com_p2', 'smoothness_xy_average_centroid_p1', 'smoothness_xy_average_centroid_p2', 'smoothness_p1_proximity', 'smoothness_p2_proximity']
newdf = pd.DataFrame(columns=newdfcolumns)

# check for each csv file for layer2 data
layer2dat = glob.glob(inputfol + '*.csv')
#print(layer2dat)

# loop over the csv layer 2 data
for vids in layer2dat:
     print(vids)
     # Load the CSV timeseries file
     ts = pd.read_csv(vids)
     # get the features
     videoID = os.path.basename(vids).split('_processed_data_layer2.csv')[0]
     originalsamples = len(ts["time"])
     perc_twopersonsdetected = ts['both_tracked'].sum() / len(ts)
     # check metadata start
     start = metadata[metadata['VIDEO_ID'] == videoID]['start'].values
     print("start time of this video: " + str(start))
     # calculate the average movement of the com for each person
     average_com_movementp1 = np.mean(np.sqrt((ts['com_p1_x'].diff()**2 + ts['com_p1_y'].diff()**2)))
     average_com_movementp2 = np.mean(np.sqrt((ts['com_p2_x'].diff()**2 + ts['com_p2_y'].diff()**2)))
     # add a time adjusted variable to the dataset
     if start.size > 0: 
        # check if endtime not greater than the last time in the dataset
        if (start[0] + constantwindowsize_sec) > ts['time'].max():
            print("End time is greater than the last time in the dataset, setting end time to max value")
        timeadjusted = "TRUE - Adjusted to start at + " + str(start[0]) + "With a window length of: " + str(constantwindowsize_sec) + " seconds"
        # Take a timeseries chunk of 150 seconds
        ts = ts[(ts['time'] >= start[0]) & (ts['time'] <= start[0] + constantwindowsize_sec)]
     if start.size == 0:
        timeadjusted = "FALSE - Not adjusted to start time as no start time was given for this video, window length is still set to: " + str(constantwindowsize_sec) + " seconds"
        ts = ts[(ts['time'] <= constantwindowsize_sec)]
     adjustedsamples = len(ts["time"])
     print(timeadjusted) 
     # fps is mode timestaps per second 
     fps = 1/(ts['time'].diff().mode()[0])
     # add time start and time end
     start_time_analysiswindow = start[0] if start.size > 0 else 0
     end_time_analysiswindow = start_time_analysiswindow + constantwindowsize_sec
     # calculate the smoothness of the distance between the com and centroid
     smoothness_distancecom = dimensionless_squared_jerk_from_position(ts['distance_com'].values, ts['time'].values)
     # take the derivative of the distance
     distancecomspeed = np.gradient(ts['distance_com'].values, ts['time'].values)
     SPARCsmoothness_distancecom = spectral_arclength(distancecomspeed, 1/fps, padlevel=4, fc=10.0, amp_th=0.05)
     # it is the first value of the distancecomspeed
     SPARCsmoothness_distancecom = SPARCsmoothness_distancecom[0]
     #print(smoothness_distancecom)
     smoothness_distancecentroid = dimensionless_squared_jerk_from_position(ts['distance'].values, ts['time'].values)
     # calculate the smoothness of the xy positions for each person
     smoothness_xy_average_com_p1 = (dimensionless_squared_jerk_from_position(ts['com_p1_x'],ts['time'].values)+dimensionless_squared_jerk_from_position(ts['com_p1_y'],ts['time'].values))/2
     smoothness_xy_average_com_p2 = (dimensionless_squared_jerk_from_position(ts['com_p2_x'],ts['time'].values)+dimensionless_squared_jerk_from_position(ts['com_p2_y'],ts['time'].values))/2
     smoothness_xy_average_centroid_p1 = (dimensionless_squared_jerk_from_position(ts['centroid_p1_x'],ts['time'].values)+dimensionless_squared_jerk_from_position(ts['centroid_p1_y'],ts['time'].values))/2
     smoothness_xy_average_centroid_p2 = (dimensionless_squared_jerk_from_position(ts['centroid_p2_x'],ts['time'].values)+dimensionless_squared_jerk_from_position(ts['centroid_p2_y'],ts['time'].values))/2
     # calculate the smoothness of the proximity approach
     smoothness_p1_proximity = dimensionless_squared_jerk_from_position(ts['p1_com_approach_pos'].values, ts['time'].values)
     smoothness_p2_proximity = dimensionless_squared_jerk_from_position(ts['p2_com_approach_pos'].values, ts['time'].values)
     # append to the new df using concat
     newdf = pd.concat([newdf, pd.DataFrame([[videoID, timeadjusted, originalsamples, adjustedsamples, start_time_analysiswindow, end_time_analysiswindow, perc_twopersonsdetected, average_com_movementp1, average_com_movementp2, smoothness_distancecom, SPARCsmoothness_distancecom, smoothness_distancecentroid, smoothness_xy_average_com_p1, smoothness_xy_average_com_p2, smoothness_xy_average_centroid_p1, smoothness_xy_average_centroid_p2, smoothness_p1_proximity, smoothness_p2_proximity]], columns=newdfcolumns)], ignore_index=True)

# save the new df
newdf.to_csv(outputfol + 'smoothness_data.csv', index=False, encoding='latin1')
newdf.head()
# print done
print("Done with smoothness processing pipeline!")
```
:::


::: {.callout-note .callout-installation collapse="true"}
## Example output smoothness: ðŸ“Š
For each video we will now have a smoothness feature set. 

```{python examplecsvfilelayer20}
import pandas as pd
import glob as glob
import os

# Load the CSV file
csv_file = "./dataoutput_STEP2_features/smoothness_data.csv"
df = pd.read_csv(csv_file)

# Display the first few rows of the DataFrame
print(df.head())
```
:::


### Extract features for all videos using tsfresh
To show how easy it is to extract features from the time series data, we use the `tsfresh` library [@christTimeSeriesFeatuRe2018]. This library contains a large number of features designed to characterize time series data. While we do not use them in our analysis, it provides a convenient example of how easy it is to extract features from the created time series in a data-driven way. The user can of course change this code to add more features or to extract only a subset of features.


::: {.callout-note .callout-installation collapse="true"}
## Extracting more features example tsfresh ðŸ´

```{python, code_folding="hide"}
import pandas as pd
import glob
import os
from tsfresh import extract_features

# Initialize list to store features from each file
all_features = []

timeseries = glob.glob('./dataoutput_STEP1_2_timeseries/*_processed_data_layer2.csv')
# for checking lets do the first 2 time series
timeseries = timeseries[:2] # comment this line to process all files

# Process each file individually
for file in timeseries:
    ts_data = pd.read_csv(file)
    video_id = file.split('/')[-1].split('_processed')[0]
    
    # Clean NaN values for the current file
    distance_data = ts_data['distance_com'].copy()
    distance_data = distance_data.interpolate(method='linear')
    distance_data = distance_data.fillna(method='ffill')
    distance_data = distance_data.fillna(method='bfill')
    
    # Create DataFrame for current file only
    current_data = pd.DataFrame({
        'id': video_id,
        'time': range(len(distance_data)),
        'distance_com': distance_data
    })
    
    # Extract features for current file only
    current_features = extract_features(current_data, column_id='id', column_sort='time')
    
    # Add to our collection
    all_features.append(current_features)
    
    # Optional: print progress
    print(f"Processed {video_id} - extracted {len(current_features.columns)} features")
    
    # Clear variables to free memory
    del ts_data, distance_data, current_data, current_features

# Combine all features at the end
combined_features = pd.concat(all_features, ignore_index=True)

print(f"Total: Extracted {len(combined_features.columns)} features from {len(combined_features)} videos")
combined_features.to_csv('./dataoutput_STEP2_features/tsfresh_features_fromCOMdistance.csv')
```

:::


::: {.callout-note .callout-installation collapse="true"}
## Example output tsfresh: ðŸ“Š
For each video we will now have a smoothness feature set. 
```{python examplecsvfilelayer21}
import pandas as pd
import glob as glob
import os

# Load the CSV file
csv_file = "./dataoutput_STEP2_features/tsfresh_features_fromCOMdistance.csv"
df = pd.read_csv(csv_file)

# Display the first few rows of the DataFrame
print(df.head())
```
:::

::: {.callout-note .callout-settingup collapse="true"}
## DIY STEP 2: Feature Extraction ðŸ› ï¸


### Step 1: Install requirements
For each script we have provided a `requirements.txt` file. You can install the requirements using pip, by first navigating to the folder where the script is located and then running the following command in your terminal:

```bash
cd [yourspecificrootadress]/code_STEP2_smoothness_features/
pip install -r requirements.txt
```
### Step 2: Run the python script
Assuming you have timeseries data (`./dataoutput_STEP1_2_timeseries/`), you can run the script using the following command:

```bash
cd [yourspecificrootadress]/code_STEP2_smoothness_features/
python STEP2_featureextraction.py
```

:::

# Step 3: Non-linear time series analysis (R)



# Step 4: Statistical analysis (R)

::: {.callout-note .callout-settingup collapse="true"}
## Smoothness analysis in R 


```{r child = "../code_STEP4_statisticalanalysis/STEP4_Smoothness_analysis.Rmd"}
```




::: {.callout-note .callout-settingup collapse="true"}
## DIY: Analysis ðŸ› ï¸


:::


# References
